# from fastapi import FastAPI
# from fastapi.responses import JSONResponse
# from pydantic import BaseModel, root_validator
# from typing import Optional
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     BitsAndBytesConfig
# )
# import torch

# # ----------------------------
# # 1) ìš”ì²­ ìŠ¤í‚¤ë§ˆ ì •ì˜
# # ----------------------------
# class GenerateRequest(BaseModel):
#     prompt: Optional[str] = None
#     message: Optional[str] = None

#     max_new_tokens: int = 128
#     do_sample: bool = True
#     top_p: float = 0.9
#     temperature: float = 0.8
#     thread_id: Optional[str] = None

#     @root_validator(pre=True)
#     def ensure_text(cls, values):
#         if not values.get("prompt") and not values.get("message"):
#             raise ValueError("`prompt` ë˜ëŠ” `message` ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")
#         return values

# # ----------------------------
# # 2) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (GPU ê³ ì • ë°°ì¹˜)
# # ----------------------------
# print("Loading tokenizer and model...")
# tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-12b-it")

# model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-3-12b-it",
#     quantization_config=BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_quant_type="nf4",
#         bnb_4bit_compute_dtype=torch.float16,
#         bnb_4bit_use_double_quant=False
#     ),
#     # ëª¨ë“  ë ˆì´ì–´ë¥¼ CUDA GPU 0ë²ˆì— ì˜¬ë¦¬ë„ë¡ ê³ ì •
#     device_map={"": "cuda:0"},
#     torch_dtype=torch.float16
# )

# # pad_token_id ì„¤ì •
# model.config.pad_token_id = model.config.eos_token_id
# model.eval()

# # ë””ë°”ì´ìŠ¤ í™•ì¸ ë¡œê·¸
# print("First parameter on:", next(model.parameters()).device)

# # ----------------------------
# # 3) FastAPI ì•± ì„¤ì •
# # ----------------------------
# app = FastAPI(title="Gemma3 4bit Counseling API")

# @app.post("/generate")
# async def generate(req: GenerateRequest):
#     user_text = req.prompt or req.message

#     # ì‹œìŠ¤í…œ ì§€ì‹œ + ì»¨íŠ¸ë¡¤ í† í° í¬ë§·
#     system_prompt = (
#         "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. "
#         "ì‚¬ìš©ìì˜ ê³ ë¯¼ì„ ê³µê°í•˜ê³ , ëŒ€í™”ì— ì•Œë§ì€ ë‹µë³€ì„ "
#         "ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ í•´ì£¼ì„¸ìš”."
#     )
#     prompt = (
#         f"<start_of_turn>user\n"
#         f"{system_prompt}\n\n"
#         f"{user_text}<end_of_turn>\n"
#         f"<start_of_turn>model"
#     )

#     inputs = tokenizer(prompt, add_special_tokens=False, return_tensors="pt")
#     inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
#     input_ids = inputs["input_ids"]

#     try:
#         outputs = model.generate(
#             input_ids=input_ids,
#             max_new_tokens=req.max_new_tokens,
#             do_sample=req.do_sample,
#             top_p=req.top_p,
#             temperature=req.temperature,
#             repetition_penalty=1.2,
#             no_repeat_ngram_size=2,
#             early_stopping=True
#         )
#     except RuntimeError as e:
#         print("Sampling error, fallback to greedy:", e)
#         outputs = model.generate(
#             input_ids=input_ids,
#             max_new_tokens=req.max_new_tokens,
#             do_sample=False
#         )

#     gen_tokens = outputs[0][ input_ids.shape[1] : ]
#     result = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()

#     return {"response": result, "thread_id": req.thread_id}

# @app.exception_handler(ValueError)
# async def validation_exception_handler(request, exc: ValueError):
#     return JSONResponse(
#         status_code=400,
#         content={"detail": str(exc)}
#     )

# # ----------------------------
# # 4) ì‹¤í–‰ ì»¤ë§¨ë“œ
# # ----------------------------
# # uvicorn app:app --host 0.0.0.0 --port 8000 --reload

# import os
# import json
# import faiss
# import torch
# from typing import Optional
# from dotenv import load_dotenv
# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import JSONResponse
# from pydantic import BaseModel
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
# from sentence_transformers import SentenceTransformer
# from langchain.vectorstores.faiss import FAISS
# from langchain.schema import Document
# from langchain.docstore import InMemoryDocstore
# from langchain.embeddings import HuggingFaceEmbeddings
# import re

# # === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
# load_dotenv()

# gemma_model_name = "google/gemma-3-12b-it"
# print("ğŸ”§ Gemma-3 12B ëª¨ë¸ ë¡œë”© ì‹œì‘ (4bit ì–‘ìí™” ì ìš©)")

# # í† í¬ë‚˜ì´ì € ë¡œë“œ
# tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)

# # 4-bit ì–‘ìí™” ì„¤ì • ë° GPU ê³ ì • ë°°ì¹˜
# model = AutoModelForCausalLM.from_pretrained(
#     gemma_model_name,
#     quantization_config=BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_quant_type="nf4",
#         bnb_4bit_compute_dtype=torch.float16,
#         bnb_4bit_use_double_quant=False
#     ),
#     device_map={"": "cuda:0"},
#     torch_dtype=torch.float16
# )
# # EOS í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì„¤ì •
# model.config.pad_token_id = model.config.eos_token_id
# print("First parameter on:", next(model.parameters()).device)

# # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì • (generator ì‚¬ìš©)
# generator = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=128,
#     temperature=0.8,
#     top_p=0.9,
#     do_sample=True
# )
# print("âœ… ëª¨ë¸ ë¡œë”© ë° ì–‘ìí™” ì™„ë£Œ")

# # === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
# print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
# embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)
# docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
# index_to_docstore_id = {i: str(i) for i in range(len(docs))}
# docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})
# vectorstore = FAISS(embedding_model.embed_query, index, docstore, index_to_docstore_id)
# retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
# print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# # === [3] FastAPI ì•± ì •ì˜ ===
# app = FastAPI()
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # ìš”ì²­ ë°”ë”” ìŠ¤í‚¤ë§ˆ
# class GenerateRequest(BaseModel):
#     message: str
#     thread_id: Optional[str] = None

# # ê³µí†µ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# def generate_response(user_text: str) -> str:
#     # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
#     prompt = f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬íˆ ì¶”ë¡ í•˜ê³ , ëŒ€í™”ì— ì•Œë§ì€ ë‹µë³€ì„ ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì œê³µí•´ ì¤˜.

# [ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]
# {user_text}
# """
#     print("â–¶ Prompt:\n", prompt)

#     generated = generator(prompt)[0]["generated_text"]
#     print("â–¶ Raw output:\n", repr(generated))

#     # '[ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]' íƒœê·¸ ë’¤ë§Œ ì¶”ì¶œ
#     match = re.search(r"\[ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€\]([\s\S]*)", generated)
#     if match:
#         answer = match.group(1).strip()
#     else:
#         answer = generated.replace(prompt, "").strip()
#     print("âœ” Final reply:\n", answer)
#     return answer

# # === [4] '/generate' ì—”ë“œí¬ì¸íŠ¸ ===
# @app.post("/generate")
# async def generate_endpoint(req: GenerateRequest):
#     if not req.message.strip():
#         return JSONResponse(status_code=400, content={"detail": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."})
#     reply = generate_response(req.message)
#     return {"response": reply, "thread_id": req.thread_id}

# # === [5] '/chat' ì—”ë“œí¬ì¸íŠ¸ (RAG) ===
# @app.post("/chat")
# async def chat_endpoint(req: GenerateRequest):
#     query = req.message.strip()
#     if not query:
#         return JSONResponse(content={"reply":"ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™","sources":[]})
#     # RAG ê²€ìƒ‰
#     docs = retriever.get_relevant_documents(query)
#     context = "\n\n".join(doc.page_content for doc in docs)
#     # í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬íˆ ì¶”ë¡ í•˜ê³ , ëŒ€í™”ì— ì•Œë§ì€ ë‹µë³€ì„ ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì œê³µí•´ ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]
# {query}
# """
#     print("â–¶ RAG Prompt:\n", prompt)
#     generated = generator(prompt)[0]["generated_text"]
#     print("â–¶ RAG Raw output:\n", repr(generated))
#     match = re.search(r"\[ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€\]([\s\S]*)", generated)
#     if match:
#         answer = match.group(1).strip()
#     else:
#         answer = generated.replace(prompt, "").strip()
#     sources = [{"content":doc.page_content,"metadata":doc.metadata} for doc in docs]
#     return {"reply":answer, "sources":sources}

# # === [6] í—¬ìŠ¤ ì²´í¬ ===
# @app.get("/health")
# @app.get("/")
# async def health_check():
#     return {"status":"ok"}

# # === [7] ì‹¤í–‰ ===
# # uvicorn app:app --host 0.0.0.0 --port 8000 --reload

import os
import torch
import logging
from typing import Optional
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import re
import traceback

# === [0] ë¡œê¹… ì„¤ì • ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler("gemma_api.log")]
)
logger = logging.getLogger("gemma-api")

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
load_dotenv()

gemma_model_name = "google/gemma-3-12b-it"
logger.info("ğŸ”§ Gemma-3 12B ëª¨ë¸ ë¡œë”© ì‹œì‘ (4bit ì´ì¤‘ ì–‘ìí™”)")

try:
    # í† í¬ë‚˜ì´ì € ë¡œë“œ - í•œê¸€ ì§€ì›
    tokenizer = AutoTokenizer.from_pretrained(
        gemma_model_name,
        trust_remote_code=True,
        use_fast=True
    )

    # 4-bit ì´ì¤‘ ì–‘ìí™” ì„¤ì •
    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True
    )
    # ëª¨ë¸ ë¡œë“œ (from_pretrained)
    model = AutoModelForCausalLM.from_pretrained(
        gemma_model_name,
        quantization_config=quant_cfg,
        device_map="auto",            # GPUì— ë¶„ì‚° ë¡œë“œ
        offload_folder="./offload",     # í•„ìš” ì‹œ CPU ì˜¤í”„ë¡œë”©
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    model.config.pad_token_id = model.config.eos_token_id
    logger.info(f"Model loaded on devices: {model.device_map if hasattr(model, 'device_map') else 'cuda:0'}")

    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        gm = torch.cuda.get_device_properties(0).total_memory / 1e9
        am = torch.cuda.memory_allocated(0) / 1e9
        logger.info(f"GPU memory: {gm:.2f} GB, allocated: {am:.2f} GB")

    logger.info("âœ… ëª¨ë¸ ë¡œë”© ë° ì–‘ìí™” ì™„ë£Œ")
except Exception as e:
    logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    logger.error(traceback.format_exc())
    raise

# === [2] FastAPI ì•± ì •ì˜ ===
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [3] ìš”ì²­ ë°”ë”” ìŠ¤í‚¤ë§ˆ ===
class GenerateRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None

# === [4] ì§ì ‘ ìƒì„± í•¨ìˆ˜ ===
def generate_direct(user_text: str, max_new_tokens: int = 256) -> str:
    safe_text = user_text.strip()
    prompt_text = (
        "ë„ˆëŠ” ì¹œì ˆí•œ AI ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìì˜ ë§ì„ ê³µê°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì¤˜.\n\n"
        f"ì‚¬ìš©ì: {safe_text}\n"
        "ì±—ë´‡: "
    )
    logger.info(f"â–¶ Prompt: {prompt_text[:60]}...")

    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)

    # 1) ìƒ˜í”Œë§ ìƒì„±
    try:
        logger.info("ìƒ˜í”Œë§ ì‹œë„")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
    except RuntimeError as e:
        logger.warning(f"ìƒ˜í”Œë§ ì‹¤íŒ¨({e}), ë¹” ì„œì¹˜ë¡œ í´ë°±")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                num_beams=4,
                early_stopping=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
    
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    logger.info(f"â–¶ Full output: {full_output[:80]}...")
    print(full_output)
    # ì‘ë‹µ ì¶”ì¶œ ë° ì •ì œ
    if "ì±—ë´‡:" in full_output:
        response = full_output.split("ì±—ë´‡:", 1)[1].strip()
    else:
        response = full_output[len(prompt_text):].strip()

    # íŠ¹ìˆ˜ í† í° ì œê±°
    response = re.sub(r"<unused\d+>", "", response).strip()

    if not response:
        logger.warning("ë¹ˆ ì‘ë‹µ, ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©")
        response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    logger.info(f"âœ” Reply: {response[:60]}...")
    return response

# === [5] '/generate' ì—”ë“œí¬ì¸íŠ¸ ===
@app.post("/generate")
async def generate_endpoint(request: Request):
    try:
        body = await request.json()
        msg = body.get("message", "").strip()
        tid = body.get("thread_id")
        if not msg:
            return JSONResponse(status_code=400, content={"detail": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."})
        logger.info(f"ìš”ì²­ ë©”ì‹œì§€: {msg[:50]}...")
        reply = generate_direct(msg)
        return JSONResponse(content={"response": reply, "thread_id": tid})
    except Exception as e:
        logger.error(f"ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return JSONResponse(status_code=500, content={"detail": "ì„œë²„ ì˜¤ë¥˜"})

# === [6] í—¬ìŠ¤ ì²´í¬ ë° ì •ë³´ ===
@app.get("/health")
async def health():
    return JSONResponse(content={"status": "ok", "model": gemma_model_name})

@app.get("/info")
async def info():
    data = {
        "model": gemma_model_name,
        "quantization": "4-bit double",
        "device_map": model.device_map if hasattr(model, 'device_map') else 'auto',
        "vocab_size": len(tokenizer)
    }
    return JSONResponse(content=data)

# === [7] ì„œë²„ ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"ì„œë²„ ì‹œì‘: 0.0.0.0:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")
