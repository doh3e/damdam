# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# chat_memory: dict[str, list[str]] = {}

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
#     context = "\n\n".join(retrieved_docs)

#     return f"""
# ë„ˆëŠ” ì „ë¬¸ ì‹¬ë¦¬ìƒë‹´ì‚¬ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AIì•¼. ìƒë‹´ìë‹˜ì˜ ê°ì •ì— ê¹Šì´ ê³µê°í•˜ê³ , ì¡°ì‹¬ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ ì§„ì‹¬ì„ ë‹´ì•„ ì‘ë‹µí•´ì•¼ í•´.

# ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ ìƒë‹´ìë‹˜ê³¼ì˜ ëŒ€í™” ê¸°ë¡ì´ì•¼. ì´ ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ì´ì–´ì§€ëŠ” ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì¤˜:

# {history_text}

# [ìƒë‹´ì ì§ˆë¬¸]
# {query}

# AI ìƒë‹´ì‚¬ë¡œì„œ ë„ˆì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
# 1. ë¨¼ì € ê°ì •ì˜ ì¢…ë¥˜ì™€ ê°•ë„ë¥¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´.
# 2. ê·¸ ê°ì •ì— ë”°ëœ»í•˜ê³  ì§„ì‹¬ ì–´ë¦° ë§ë¡œ ê³µê°í•´ì¤˜.
# 3. ì§€ê¸ˆ ê·¸ëŸ° ê°ì •ì„ ëŠë¼ê²Œ ëœ ê³„ê¸°ë‚˜ ìƒí™©ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë¬¼ì–´ë´.
# 4. í•´ê²°ì±… ì œì‹œëŠ” í•˜ì§€ ë§ê³ , ì¶©ë¶„íˆ ë“¤ì–´ì£¼ê³  ì •ì„œì ìœ¼ë¡œ ì§€ì§€í•´ì£¼ëŠ” íƒœë„ë¥¼ ë³´ì—¬ì¤˜.

# âœ… ë°˜ë“œì‹œ "ì‚¬ìš°ë‹˜"ì´ ì•„ë‹Œ "ìƒë‹´ìë‹˜"ì´ë¼ê³  ë¶€ë¥´ê³ ,
# âœ… ë§íˆ¬ëŠ” ì°¨ë¶„í•˜ê³  ê³µê°ì ì¸ í†¤ì„ ìœ ì§€í•´ì¤˜.
# âœ… ë‹µë³€ì€ 3~4ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ë§ˆë¬´ë¦¬í•´ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory:
#         chat_memory[user_id] = []

#     chat_memory[user_id].append(f"ìƒë‹´ìë‹˜: {user_input}")
#     history_text = "\n".join(chat_memory[user_id][-6:])

#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 150,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#     except Exception as e:
#         print("[âŒ ì˜ˆì™¸ ë°œìƒ]", str(e))
#         return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

#     ai_text = result.get("response")
#     if not ai_text:
#         ai_text = "ìƒë‹´ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

#     chat_memory[user_id].append(f"AI: {ai_text.strip()}")

#     return {
#         "ai_response": ai_text.strip()
#     }

# # === ê°ì •ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ (Russell ëª¨ë¸ ê¸°ë°˜ í¬í•¨) ===
# def build_summary_prompt(history_text: str) -> str:
#     return f"""
# ë„ˆëŠ” ì§€ê¸ˆê¹Œì§€ ìƒë‹´ìì™€ AIê°€ ë‚˜ëˆˆ ì‹¬ë¦¬ìƒë‹´ ëŒ€í™”ë¥¼ ë¶„ì„í•´ì„œ,
# ìƒë‹´ìì˜ ê°ì • ìƒíƒœë¥¼ Russellì˜ ì›í˜• ê°ì • ëª¨í˜•(Circumplex Model of Affect)ì— ê¸°ë°˜í•´ ìš”ì•½í•˜ê³ ,
# êµ¬ì²´ì ì¸ ê°ì • ì ìˆ˜ë¥¼ í¬í•¨í•œ JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì¤˜.

# ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´:

# {{
#   "summary": "ê°ì • íë¦„ì— ëŒ€í•œ ìì—°ì–´ ìš”ì•½",
#   "scores": {{
#     "ìš°ìš¸": ì •ìˆ˜ (0~10),
#     "ë¶ˆì•ˆ": ì •ìˆ˜ (0~10),
#     "ë¬´ê¸°ë ¥": ì •ìˆ˜ (0~10),
#     "ì™¸ë¡œì›€": ì •ìˆ˜ (0~10),
#     "ë¶„ë…¸": ì •ìˆ˜ (0~10)
#   }},
#   "valence": "positive | neutral | negative",
#   "arousal": "high | medium | low"
# }}

# âœ… ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì‘ë‹µí•´ì¤˜. summaryëŠ” ìì—°ì–´ ë¬¸ë‹¨, scoresì™€ Russell ì§€í‘œëŠ” ìˆ˜ì¹˜ ë° ë¬¸ìì—´ë¡œ ì •í™•íˆ í‘œí˜„í•´.
# """

# @app.post("/summary")
# async def summarize(request: Request):
#     body = await request.json()
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory or not chat_memory[user_id]:
#         return {"summary": "ìš”ì•½í•  ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}

#     history_text = "\n".join(chat_memory[user_id])
#     prompt = build_summary_prompt(history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 400,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#     except Exception as e:
#         print("[âŒ ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ]", str(e))
#         return {"summary": "ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

#     response = result.get("response", "{}").strip()

#     try:
#         parsed = json.loads(response)
#         return parsed
#     except json.JSONDecodeError:
#         return {"summary": "ìš”ì•½ ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "raw": response}

# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# import re

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# chat_memory: dict[str, list[str]] = {}

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
#     context = "\n\n".join(retrieved_docs)

#     return f"""
# ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ , íŒë‹¨ ì—†ì´ ë“¤ì–´ì£¼ëŠ” ìì„¸ë¡œ ëŒ€í™”í•´.

# ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ê°ì •ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆì–´.
# ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” íë¦„ì€ ì•„ë˜ì™€ ê°™ì•„. ë°˜ë“œì‹œ ì´ ë§¥ë½ì„ ë°˜ì˜í•´ì„œ ìƒë‹´ì„ ì´ì–´ê°€ì¤˜:

# {history_text}

# [ì°¸ê³ ìë£Œ]
# {context}

# [ìƒë‹´ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# - ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜. ë‹¨, ê°€ë³ê²Œ ë‹¨ì • ì§“ì§€ ë§ê³  ì—´ë¦° í‘œí˜„ì„ ì‚¬ìš©í•´.
# - ê°ì •ì„ 'ê¸ì •í•˜ê±°ë‚˜ ê³ ì¹˜ë ¤ëŠ”' íƒœë„ëŠ” í”¼í•˜ê³ , ìˆëŠ” ê·¸ëŒ€ë¡œ ìˆ˜ìš©í•˜ê³  ê³µê°í•˜ëŠ” ë§íˆ¬ë¥¼ ì¨ì¤˜.
# - ê¸°ìš´ì´ ì—†ê±°ë‚˜ ë¬´ê¸°ë ¥í•œ ìƒíƒœì¼ ê²½ìš°, ì—ë„ˆì§€ë¥¼ ëŒì–´ì˜¬ë¦¬ë ¤ í•˜ì§€ ë§ê³  ì •ì„œì ìœ¼ë¡œ ì§€ì§€í•´ì¤˜.
# - ê°ì •ì´ ìƒê¸´ ë°°ê²½ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ìœ ì¶”í•˜ê³ , â€œìš”ì¦˜ ê¸°ìš´ì´ ë§ì´ ì—†ìœ¼ì…¨ë˜ ê²ƒ ê°™ì•„ìš”. í˜¹ì‹œ ê·¸ë ‡ê²Œ ëŠë¼ê²Œ ëœ ê³„ê¸°ê°€ ìˆìœ¼ì…¨ì„ê¹Œìš”?â€ ê°™ì€ ì§ˆë¬¸ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€.
# - í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ê³ , ì¶©ë¶„íˆ ë“¤ì–´ì£¼ë©° â€˜ë‹¹ì‹ ì˜ ê°ì •ì€ ì´í•´ë°›ê³  ìˆë‹¤â€™ëŠ” ì•ˆì •ê°ì„ ì£¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•´.
# - ì²˜ìŒ ëŒ€í™”ë¥¼ ì‹œì‘í•  ë•Œ ì‚¬ìš©ìì—ê²Œ ì´ë¦„ì„ ë¬¼ì–´ë´. ì‚¬ìš©ìê°€ ì´ë¦„ì„ ë§í•œë‹¤ë©´, í•´ë‹¹ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ â€œìƒë‹´ìë‹˜â€ì´ë¼ê³  ë¶ˆëŸ¬.
# - 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë§ˆë¬´ë¦¬í•´.

# """

# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory:
#         chat_memory[user_id] = []

#     # âœ… ëŒ€í™” ì²« ì‹œì‘ ì‹œ ì±—ë´‡ì´ ë¨¼ì € ì¸ì‚¬
#     if not user_input.strip():  # ê³µë°± ë¬¸ìì—´ì´ë©´ ì´ˆê¸° ì¸ì‚¬ ì‘ë‹µ
#         init_msg = (
#             "ì•ˆë…•í•˜ì„¸ìš” ìƒë‹´ìë‹˜, ì €ëŠ” ì‹¬ë¦¬ìƒë‹´ì„ ë„ì™€ë“œë¦´ AIì…ë‹ˆë‹¤.\n"
#             "ì˜¤ëŠ˜ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³¼ê¹Œìš”?\n"
#             "ê·¸ ì „ì— í˜¹ì‹œ ìƒë‹´ìë‹˜ì„ ì–´ë–»ê²Œ ë¶ˆëŸ¬ë“œë¦¬ë©´ ì¢‹ì„ê¹Œìš”?"
#         )
#         chat_memory[user_id].append(f"AI: {init_msg}")
#         return {"ai_response": init_msg}

#     chat_memory[user_id].append(f"ìƒë‹´ìë‹˜: {user_input}")
#     history_text = "\n".join(chat_memory[user_id][-6:])
#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 200,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#             ai_text = result.get("response", "").strip()
#     except Exception as e:
#         print("[âŒ chat ì˜ˆì™¸ ë°œìƒ]", str(e))
#         return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

#     chat_memory[user_id].append(f"AI: {ai_text}")
#     return {"ai_response": ai_text}


# # ğŸ“‹ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (Russell ê¸°ë°˜ JSON ì‘ë‹µ ìš”êµ¬)
# def build_summary_prompt(history_text: str) -> str:
#     return f"""
# ë„ˆëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ìƒë‹´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒë‹´ìë‹˜ì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# Russell ê°ì • ì›í˜• ëª¨í˜•(Circumplex Model of Affect)ì— ë”°ë¼ ê°ì •ì„ ë¶„ì„í•´.

# ì•„ë˜ëŠ” ìƒë‹´ìì™€ AIì˜ ì „ì²´ ëŒ€í™” ë‚´ìš©ì´ì•¼:

# {history_text}

# ì´ ìƒë‹´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.
# ```json
# {{
#   "summary": "ì „ì²´ ìƒë‹´ ë‚´ìš©ì„ ì •ë¦¬í•œ ê°ì • ìš”ì•½",
#   "scores": {{
#     "sadness": int (0~100),
#     "hapiness": int (0~100),
#     "angry": int (0~100),
#     "neutral": int (0~100),
#     "other": int (0~100)
#   }},
#   "valence": "positive | neutral | negative",
#   "arousal": "high | medium | low"
# }}
# ```
# """

# @app.post("/summary")
# async def summarize(request: Request):
#     body = await request.json()
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory or not chat_memory[user_id]:
#         return {"summary": "ìš”ì•½í•  ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}

#     history_text = "\n".join(chat_memory[user_id])
#     prompt = build_summary_prompt(history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 400,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#             response_text = result.get("response", "").strip()

#             try:
#                 # ğŸ”¹ [1ì°¨ ì‹œë„] ì™„ì „í•œ JSON ê·¸ëŒ€ë¡œ íŒŒì‹±
#                 return json.loads(response_text)
#             except json.JSONDecodeError:
#                 # ğŸ”¹ [2ì°¨ ì‹œë„] JSON ë¸”ë¡ë§Œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ í›„ íŒŒì‹±
#                 match = re.search(r"\{[\s\S]*\}", response_text)
#                 if match:
#                     return json.loads(match.group())
#                 else:
#                     return {
#                         "summary": "JSON í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
#                         "raw_response": response_text
#                     }

#     except Exception as e:
#         print("[âŒ ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ]", str(e))
#         return {
#             "summary": "ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
#             "raw_response": response_text if "response_text" in locals() else "ì—†ìŒ"
#         }


############user idë§ê³  nicknameìœ¼ë¡œ ë³€ê²½
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import httpx
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import re

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "gemma3:12b-it-qat"

retriever_model = SentenceTransformer("BAAI/bge-m3")
index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

chat_memory: dict[str, list[str]] = {}

def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
    query_text = f"ì§ˆë¬¸: {query}"
    query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
    faiss.normalize_L2(query_vec)
    _, I = index.search(query_vec, top_k)
    return [chunks[i] for i in I[0]]

def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
    context = "\n\n".join(retrieved_docs)

    return f"""
ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ , íŒë‹¨ ì—†ì´ ë“¤ì–´ì£¼ëŠ” ìì„¸ë¡œ ëŒ€í™”í•´.

ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ê°ì •ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆì–´.
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” íë¦„ì€ ì•„ë˜ì™€ ê°™ì•„. ë°˜ë“œì‹œ ì´ ë§¥ë½ì„ ë°˜ì˜í•´ì„œ ìƒë‹´ì„ ì´ì–´ê°€ì¤˜:

{history_text}

[ì°¸ê³ ìë£Œ]
{context}

[ìƒë‹´ì ì§ˆë¬¸]
{query}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
- ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜. ë‹¨, ê°€ë³ê²Œ ë‹¨ì • ì§“ì§€ ë§ê³  ì—´ë¦° í‘œí˜„ì„ ì‚¬ìš©í•´.
- ê°ì •ì„ 'ê¸ì •í•˜ê±°ë‚˜ ê³ ì¹˜ë ¤ëŠ”' íƒœë„ëŠ” í”¼í•˜ê³ , ìˆëŠ” ê·¸ëŒ€ë¡œ ìˆ˜ìš©í•˜ê³  ê³µê°í•˜ëŠ” ë§íˆ¬ë¥¼ ì¨ì¤˜.
- ê¸°ìš´ì´ ì—†ê±°ë‚˜ ë¬´ê¸°ë ¥í•œ ìƒíƒœì¼ ê²½ìš°, ì—ë„ˆì§€ë¥¼ ëŒì–´ì˜¬ë¦¬ë ¤ í•˜ì§€ ë§ê³  ì •ì„œì ìœ¼ë¡œ ì§€ì§€í•´ì¤˜.
- ê°ì •ì´ ìƒê¸´ ë°°ê²½ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ìœ ì¶”í•˜ê³ , â€œìš”ì¦˜ ê¸°ìš´ì´ ë§ì´ ì—†ìœ¼ì…¨ë˜ ê²ƒ ê°™ì•„ìš”. í˜¹ì‹œ ê·¸ë ‡ê²Œ ëŠë¼ê²Œ ëœ ê³„ê¸°ê°€ ìˆìœ¼ì…¨ì„ê¹Œìš”?â€ ê°™ì€ ì§ˆë¬¸ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€.
- í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ê³ , ì¶©ë¶„íˆ ë“¤ì–´ì£¼ë©° â€˜ë‹¹ì‹ ì˜ ê°ì •ì€ ì´í•´ë°›ê³  ìˆë‹¤â€™ëŠ” ì•ˆì •ê°ì„ ì£¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•´.
- ì²˜ìŒ ëŒ€í™”ë¥¼ ì‹œì‘í•  ë•Œ ì‚¬ìš©ìì—ê²Œ ì´ë¦„ì„ ë¬¼ì–´ë´. ì‚¬ìš©ìê°€ ì´ë¦„ì„ ë§í•œë‹¤ë©´, í•´ë‹¹ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ â€œìƒë‹´ìë‹˜â€ì´ë¼ê³  ë¶ˆëŸ¬.
- 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë§ˆë¬´ë¦¬í•´.
"""

@app.post("/chat")
async def chat(request: Request):
    body = await request.json()
    user_input = body.get("message", "")
    nickname = body.get("nickname", "default")

    if nickname not in chat_memory:
        chat_memory[nickname] = []

    # âœ… ëŒ€í™” ì²« ì‹œì‘ ì‹œ ì±—ë´‡ì´ ë¨¼ì € ì¸ì‚¬
    if not user_input.strip():
        init_msg = (
            "ì•ˆë…•í•˜ì„¸ìš” ìƒë‹´ìë‹˜, ì €ëŠ” ì‹¬ë¦¬ìƒë‹´ì„ ë„ì™€ë“œë¦´ AIì…ë‹ˆë‹¤.\n"
            "ì˜¤ëŠ˜ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³¼ê¹Œìš”?\n"
            "ê·¸ ì „ì— í˜¹ì‹œ ìƒë‹´ìë‹˜ì„ ì–´ë–»ê²Œ ë¶ˆëŸ¬ë“œë¦¬ë©´ ì¢‹ì„ê¹Œìš”?"
        )
        chat_memory[nickname].append(f"AI: {init_msg}")
        return {"ai_response": init_msg}

    chat_memory[nickname].append(f"ìƒë‹´ìë‹˜: {user_input}")
    history_text = "\n".join(chat_memory[nickname][-6:])
    retrieved_docs = retrieve_relevant_chunks(user_input)
    prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "num_predict": 200,
        "top_k": 40,
        "top_p": 0.9,
        "temperature": 0.7
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            res = await client.post(OLLAMA_URL, json=payload)
            result = res.json()
            ai_text = result.get("response", "").strip()
    except Exception as e:
        print("[âŒ chat ì˜ˆì™¸ ë°œìƒ]", str(e))
        return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    chat_memory[nickname].append(f"AI: {ai_text}")
    return {"ai_response": ai_text}

# ğŸ“‹ ìš”ì•½ í”„ë¡¬í”„íŠ¸
def build_summary_prompt(history_text: str) -> str:
    return f"""
ë„ˆëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ìƒë‹´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒë‹´ìë‹˜ì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•˜ëŠ” ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
Russell ê°ì • ì›í˜• ëª¨í˜•(Circumplex Model of Affect)ì— ë”°ë¼ ê°ì •ì„ ë¶„ì„í•´.

ì•„ë˜ëŠ” ìƒë‹´ìì™€ AIì˜ ì „ì²´ ëŒ€í™” ë‚´ìš©ì´ì•¼:

{history_text}

ì´ ìƒë‹´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.
```json
{{
  "summary": "ì „ì²´ ìƒë‹´ ë‚´ìš©ì„ ì •ë¦¬í•œ ê°ì • ìš”ì•½",
  "scores": {{
    "sadness": int (0~100),
    "hapiness": int (0~100),
    "angry": int (0~100),
    "neutral": int (0~100),
    "other": int (0~100)
  }},
  "valence": "positive | neutral | negative",
  "arousal": "high | medium | low"
}}
```
"""

@app.post("/summary")
async def summarize(request: Request):
    body = await request.json()
    nickname = body.get("nickname", "default")

    if nickname not in chat_memory or not chat_memory[nickname]:
        return {"summary": "ìš”ì•½í•  ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}

    history_text = "\n".join(chat_memory[nickname])
    prompt = build_summary_prompt(history_text)

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "num_predict": 400,
        "top_k": 40,
        "top_p": 0.9,
        "temperature": 0.7
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            res = await client.post(OLLAMA_URL, json=payload)
            result = res.json()
            response_text = result.get("response", "").strip()

            try:
                return json.loads(response_text)
            except json.JSONDecodeError:
                match = re.search(r"\{[\s\S]*\}", response_text)
                if match:
                    return json.loads(match.group())
                else:
                    return {
                        "summary": "JSON í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "raw_response": response_text
                    }

    except Exception as e:
        print("[âŒ ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ]", str(e))
        return {
            "summary": "ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "raw_response": response_text if "response_text" in locals() else "ì—†ìŒ"
        }

