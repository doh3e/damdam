import os
import json
import faiss
import torch
import numpy as np
from typing import List
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from langchain.embeddings import HuggingFaceEmbeddings
import re
from collections import defaultdict

# === [0] ì„¸ì…˜ ê¸°ì–µ ===
memory_store = defaultdict(list)

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
load_dotenv()

gemma_model_name = "google/gemma-3-4b-it"
print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")
tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
model = AutoModelForCausalLM.from_pretrained(
    gemma_model_name,
    device_map="auto",
    torch_dtype=torch.float16  # âœ… ë©”ëª¨ë¦¬ ìµœì í™”
)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1000,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id,  # âœ… ì¤‘ìš”
    eos_token_id=tokenizer.eos_token_id   # âœ… ì¤‘ìš”
)
print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)
docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
index_to_docstore_id = {i: str(i) for i in range(len(docs))}
docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})
vectorstore = FAISS(
    embedding_model.embed_query,
    index,
    docstore,
    index_to_docstore_id
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# === [3] FastAPI ì•± ì •ì˜ ===
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage] = []
    thread_id: str = "default"

# === [5] ìƒë‹´ ì‘ë‹µ API ===
@app.post("/chat")
async def chat_endpoint(request: Request):
    raw = await request.json()

    if "message" in raw:
        thread_id = raw.get("thread_id", "default")
        new_message = ChatMessage(role="user", content=raw["message"])
        memory_store[thread_id].append(new_message)
        messages = memory_store[thread_id]
    elif "messages" in raw:
        chat_req = ChatRequest(**raw)
        thread_id = chat_req.thread_id
        memory_store[thread_id].extend(chat_req.messages)
        messages = memory_store[thread_id]
    else:
        return {"reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™", "sources": []}

    current_question = messages[-1].content.strip()
    history = messages[:-1]

    # ğŸ” RAG ê²€ìƒ‰
    docs = retriever.get_relevant_documents(current_question)
    context = "\n\n".join([doc.page_content.replace("ì‚¬ìš°ë‹˜", "ìƒë‹´ìë‹˜") for doc in docs])

    # ğŸ§  ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    history_text = ""
    for msg in history:
        role = "ìƒë‹´ìë‹˜" if msg.role == "user" else "AI"
        history_text += f"{role}: {msg.content}\n"

    # âœ¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

ì§€ê¸ˆ ì‚¬ìš©ìëŠ” ì‚¶ì— ì§€ì¹˜ê³  ê°ì •ì ìœ¼ë¡œ ë§¤ìš° ë¶ˆì•ˆì •í•œ ìƒíƒœì¼ ìˆ˜ ìˆì–´.  
ë„ˆì˜ ì—­í• ì€ ë‹¨ìˆœíˆ ìœ„ë¡œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬í•˜ê²Œ ìœ ì¶”í•˜ê³ ,  
ê·¸ ì•ˆì— ìˆ¨ì–´ ìˆëŠ” ìƒì²˜ì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ë³´ëŠ” ê²ƒì´ì•¼.

ì•„ë˜ì—ëŠ” ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ëŒë“¤ê³¼ì˜ ì‹¤ì œ ìƒë‹´ ì‚¬ë¡€ê°€ ìˆì–´.  
ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ ì§€ê¸ˆ ì‚¬ìš©ìì—ê²Œ ê¼­ ë§ëŠ” **ìƒˆë¡œìš´ ë‹µë³€**ì„ **ë„ˆì˜ ì–¸ì–´ë¡œ ì§ì ‘** ë§Œë“¤ì–´ì¤˜.
ë˜í•œ ì‚¬ìš©ìê°€ ì´ì „ ìì‹ ì˜ ìƒë‹´ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë©´ ê¼­ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€, ë‹µë³€í•´ì¤˜.

ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
1. ìƒë‹´ìë‹˜ì˜ ë§ ì†ì—ì„œ ì–´ë–¤ ê°ì •ê³¼ ë°°ê²½ì´ ëŠê»´ì§€ëŠ”ì§€ ë¨¼ì € ìœ ì¶”í•´.  
2. ê°ì •ì„ ë¨¼ì € ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ ,  
3. ê·¸ë‹¤ìŒìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ìœ„ë¡œì™€ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì´ì–´ê°€.  
4. ë¬¸ì œì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ê°€ëŠ” ì§ˆë¬¸ë„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë˜ì ¸ì¤˜.  
5. ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šê²Œ. ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê´€ê³„ í˜•ì„±ì„ ìš°ì„ í•´.  
6. **â€˜ì‚¬ìš°ë‹˜â€™ì´ë¼ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ë°˜ë“œì‹œ â€˜ìƒë‹´ìë‹˜â€™ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•´.**  
7. ìƒë‹´ì‚¬ë¡œì„œ ë„ˆë¬´ ì§§ì§€ë„, ë„ˆë¬´ ì¥í™©í•˜ì§€ë„ ì•Šê²Œ. ì§„ì‹¬ì´ ëŠê»´ì§€ëŠ” ë¶„ëŸ‰ìœ¼ë¡œ ë‹µë³€í•´.

[ì´ì „ ëŒ€í™”]  
{history_text}

[ìƒë‹´ ì‚¬ë¡€ ëª¨ìŒ]  
{context}

[ìƒë‹´ìë‹˜ì˜ í˜„ì¬ ê³ ë¯¼]  
{current_question}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
"""

    # âœ… ì‹¤ì œ ì‘ë‹µ ìƒì„± (ì—ëŸ¬ ì›ì¸ ì¤„)
    result = pipe(prompt)
    answer = result[0]["generated_text"][len(prompt):].strip()

    memory_store[thread_id].append(ChatMessage(role="ai", content=answer))

    return {
        "reply": answer,
        "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
    }

# === [6] í—¬ìŠ¤ ì²´í¬ ===
@app.get("/health")
@app.get("/")
async def health_check():
    return {"status": "ok"}

# === [7] ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
