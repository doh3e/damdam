import os
import torch
import logging
import traceback
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

# === [0] ë¡œê¹… ì„¤ì • ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("api_server.log")
    ]
)
logger = logging.getLogger("gemma-api")

# === [1] ëª¨ë¸ëª… ë° ì„¤ì • ===
model_name = "google/gemma-3-4b-it"

# === [2] í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë”© ===
logger.info("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
try:
    # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ - í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì • ì¶”ê°€
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,  # ì›ê²© ì½”ë“œ ì‹ ë¢° ì„¤ì •
        use_fast=True            # ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
    )
    
    # ëª¨ë¸ ë¡œë“œ - ì•ˆì •ì„±ì„ ìœ„í•œ ì„¤ì • ì¶”ê°€
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,  # ì–‘ìí™” ëŒ€ì‹  float16 ì‚¬ìš©
        device_map="auto",
        attn_implementation="eager"  # ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ ë³€ê²½
    )
    
    # ëª¨ë¸ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    # ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
        logger.info(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f}GB, í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated_memory:.2f}GB")
        logger.info(f"CUDA ë²„ì „: {torch.version.cuda}")
        logger.info(f"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
    
    logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    logger.error(traceback.format_exc())
    raise

# === [3] FastAPI ì•± ì •ì˜ ===
app = FastAPI(title="Gemma 3 API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ìŠ¤í‚¤ë§ˆ ì •ì˜ ===
class ChatInput(BaseModel):
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
    
    class Config:
        json_schema_extra = {
            "example": {
                "message": "ì•ˆë…•í•˜ì„¸ìš”"
            }
        }

# === [5] ì—ëŸ¬ í•¸ë“¤ëŸ¬ ===
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(exc)}")
    logger.error(traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={"detail": f"ì„œë²„ ì˜¤ë¥˜: {str(exc)}"},
        media_type="application/json; charset=utf-8"
    )

# === [6] ì§ì ‘ ìƒì„± í•¨ìˆ˜ (ì•ˆì •ì„± ê°œì„ ) ===
def generate_text(prompt, max_length=500):
    try:
        logger.info(f"í”„ë¡¬í”„íŠ¸: {prompt}")
        # ì…ë ¥ í† í°í™” - ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì§€ì •
        inputs = tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ì•ˆì „í•œ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        with torch.no_grad():
            try:
                # ì²« ë²ˆì§¸ ì‹œë„: ìƒ˜í”Œë§ ì—†ì´ ì•ˆì „í•˜ê²Œ ìƒì„±
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=False,  # ìƒ˜í”Œë§ ë¹„í™œì„±í™” (greedy decoding ì‚¬ìš©)
                    num_beams=1,      # beam search ë¹„í™œì„±í™”
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
                logger.info("ì²« ë²ˆì§¸ ìƒì„± ì‹œë„ ì„±ê³µ")
            except RuntimeError as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                logger.warning(f"ì²« ë²ˆì§¸ ìƒì„± ì‹œë„ ì‹¤íŒ¨: {str(e)}, ì•ˆì „ ëª¨ë“œë¡œ ì¬ì‹œë„")
                try:
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=max_length,
                        do_sample=False,
                        num_beams=1,
                        use_cache=False,  # ìºì‹œ ì‚¬ìš© ë¹„í™œì„±í™”
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    logger.info("ë‘ ë²ˆì§¸ ìƒì„± ì‹œë„ ì„±ê³µ")
                except RuntimeError as e2:
                    # ë‘ ë²ˆì§¸ ì‹œë„ë„ ì‹¤íŒ¨í•˜ë©´ ë” ì§§ì€ ì‘ë‹µ ìƒì„± ì‹œë„
                    logger.warning(f"ë‘ ë²ˆì§¸ ìƒì„± ì‹œë„ ì‹¤íŒ¨: {str(e2)}, ì§§ì€ ì‘ë‹µ ìƒì„± ì‹œë„")
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=100,  # ë” ì§§ì€ ì‘ë‹µ
                        do_sample=False,
                        num_beams=1,
                        use_cache=False,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    logger.info("ì„¸ ë²ˆì§¸ ìƒì„± ì‹œë„ ì„±ê³µ")
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì…ë ¥ê³¼ ì¶œë ¥ ë¡œê¹… - ë””ë²„ê¹…ìš©
        logger.info(f"ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸: {generated_text}")
        
        # í”„ë¡¬í”„íŠ¸ ì œê±° ë¡œì§ ê°œì„  - ìƒë‹´ì‚¬: ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        if "ìƒë‹´ì‚¬:" in generated_text:
            response = generated_text.split("ìƒë‹´ì‚¬:", 1)[1].strip()
        else:
            # í”„ë¡¬í”„íŠ¸ ì „ì²´ê°€ í¬í•¨ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ì¡´ ë°©ì‹ ëŒ€ì²´
            logger.warning("ìƒë‹´ì‚¬: ì ‘ë‘ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì „ì²´ ì‘ë‹µ ì‚¬ìš©")
            response = generated_text[len(prompt):].strip()
            
            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not response:
                logger.warning("ì‘ë‹µì´ ë¹„ì–´ìˆìŒ, í”„ë¡¬í”„íŠ¸ ì œê±° ì—†ì´ ì „ì²´ ì‘ë‹µ ì‚¬ìš©")
                response = generated_text.strip()
        
        logger.info(f"ìµœì¢… ì‘ë‹µ (ì²˜ìŒ 50ì): {response[:50]}...")
        return response
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        return f"ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

# === [7] /chat ì—”ë“œí¬ì¸íŠ¸ ===
@app.post("/chat")
async def chat(request: Request):
    try:
        # ì›ì‹œ ë°”ë”” ë°ì´í„°ë¥¼ ì§ì ‘ ì²˜ë¦¬
        body = await request.body()
        logger.info(f"ì›ì‹œ ìš”ì²­ ë°”ë””: {body}")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            import json
            body_json = json.loads(body)
            message = body_json.get("message", "")
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤")
        
        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not message:
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
        # ì…ë ¥ ë¡œê¹…
        logger.info(f"ì…ë ¥ ë©”ì‹œì§€: {message}")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ì‹¬ë¦¬ìƒë‹´ì‚¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        prompt = f"ë‹¹ì‹ ì€ ê³µê°ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì„ ì´í•´í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.\n\nì‚¬ìš©ì: {message}\nìƒë‹´ì‚¬:"
        
        # ì§ì ‘ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
        answer = generate_text(prompt)
        
        # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not answer or answer.isspace():
            logger.warning("ë¹ˆ ì‘ë‹µì´ ìƒì„±ë¨. ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©")
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ëª…ì‹œì  ì‘ë‹µ
        return JSONResponse(
            content={"reply": answer},
            media_type="application/json; charset=utf-8"
        )
    except HTTPException:
        # ì´ë¯¸ ìƒì„±ëœ HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
        raise
    except Exception as e:
        logger.error(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        # ë” ìì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        return JSONResponse(
            status_code=500,
            content={"detail": f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}", "type": str(type(e))},
            media_type="application/json; charset=utf-8"
        )

# === [8] í—¬ìŠ¤ ì²´í¬ ===
@app.get("/")
def health_check():
    return JSONResponse(
        content={"status": "ok", "model": model_name},
        media_type="application/json; charset=utf-8"
    )

# === [9] ëª¨ë¸ ì •ë³´ ===
@app.get("/info")
def model_info():
    info = {
        "model": model_name,
        "precision": "float16",
        "max_tokens": 500,
        "pytorch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
    }
    
    if torch.cuda.is_available():
        info["cuda_version"] = torch.version.cuda
        info["gpu_name"] = torch.cuda.get_device_name(0)
    
    return JSONResponse(
        content=info,
        media_type="application/json; charset=utf-8"
    )

# === [10] ì„œë²„ ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ í¬íŠ¸ ì„¤ì • ê°€ëŠ¥
    port = int(os.environ.get("PORT", 8000))
    
    logger.info(f"ì„œë²„ ì‹œì‘: http://0.0.0.0:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")