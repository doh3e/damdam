
######## ê°€ì¥ ê¸°ë³¸ : ì±„íŒ…ë§Œ ê°€ëŠ¥ëŠ¥
# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # ë°°í¬ ì‹œ ë„ë©”ì¸ ì œí•œ í•„ìš”
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": f"{user_input}\n\nâ¡ï¸ í•µì‹¬ë§Œ ê°„ë‹¨í•˜ê²Œ 3~4ë¬¸ì¥ìœ¼ë¡œ ì‹¬ë¦¬ìƒë‹´ ë‹µë³€í•´ì¤˜.",
#         "stream": False,
#         "num_predict": 200,  # ğŸ”§ ìµœëŒ€ ìƒì„± í† í° ìˆ˜ ì œí•œ
#         "top_k": 40,
#         "top_p": 0.9
#     }

#     async with httpx.AsyncClient(timeout=60.0) as client:
#         response = await client.post(OLLAMA_URL, json=payload)
#         result = response.json()

#     return {"response": result.get("response")}

# main.py
######### rag ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ê°€
# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer

# # === ì„¤ì • ===
# app = FastAPI()
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"], allow_credentials=True,
#     allow_methods=["*"], allow_headers=["*"]
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [1] FAISS ê²€ìƒ‰ í•¨ìˆ˜ ===
# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# # === [2] í”„ë¡¬í”„íŠ¸ ìƒì„± ===
# def build_rag_prompt(query: str, retrieved_docs: list[str]) -> str:
#     context = "\n\n".join(retrieved_docs)
#     return f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼. ê°ì •ì„ ìœ ì¶”í•˜ê³  ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ì—­í• ì„ í•´.
# ë‹¤ìŒì€ ìœ ì‚¬í•œ ê³ ë¯¼ ì‚¬ë¡€ì•¼. ì´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ëœ»í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ë‹µí•´ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [3] FastAPI ì—”ë“œí¬ì¸íŠ¸ ===
# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")

#     # [1] RAG ê²€ìƒ‰
#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs)

#     # [2] Ollama ìš”ì²­
#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 300,
#         "top_k": 40,
#         "top_p": 0.9
#     }

#     async with httpx.AsyncClient(timeout=60.0) as client:
#         res = await client.post(OLLAMA_URL, json=payload)
#         result = res.json()

#     # [3] ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì„œ ì‘ë‹µ
#     return {
#         "ai_response": result.get("response", "").strip(),
#         "retrieved_chunks": retrieved_docs
#     }

############## í”„ë¡¬í”„íŠ¸ ê³ ë„í™”
# main.py
# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer

# # === FastAPI ì„¸íŒ… ===
# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # í”„ë¡ íŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í—ˆìš©
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # === Ollama ì„¸íŒ… ===
# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# # === ë²¡í„° ê²€ìƒ‰ê¸° ë¡œë”© ===
# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [1] FAISS ê²€ìƒ‰ í•¨ìˆ˜ ===
# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# # === [2] í”„ë¡¬í”„íŠ¸ ìƒì„± (ê³ ë„í™”) ===
# def build_rag_prompt(query: str, retrieved_docs: list[str]) -> str:
#     context = "\n\n".join(retrieved_docs)

#     return f"""
# ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ì„ìƒì‹¬ë¦¬í•™ê³¼ ì •ì‹ ê±´ê°• ìƒë‹´í•™ì„ ì „ê³µí•œ ì „ë¬¸ê°€ë¡œì„œ, ê°ì •ì— ê¹Šì´ ê³µê°í•˜ë©´ì„œë„ ë¬¸ì œì˜ ë³¸ì§ˆì„ í•¨ê»˜ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•´.

# ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ë¬¸ì œë¡œ í˜ë“¤ì–´í•˜ê³  ìˆì–´.
# ìƒë‹´ìì˜ ë§ì—ì„œ ëŠê»´ì§€ëŠ” **ê°ì •ì˜ ì¢…ë¥˜ì™€ ê°•ë„**ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ê·¸ ê°ì •ì— ë§ëŠ” ë§íˆ¬ì™€ ì¡°ì–¸ ë°©ì‹ì„ íƒí•´ì¤˜.

# ë‹µë³€ êµ¬ì„±ì€ ì•„ë˜ ìˆœì„œëŒ€ë¡œ í•´:
# 1. ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜.
# 2. ê·¸ ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•´ì¤˜.
# 3. ê°ì •ì´ ìƒê¸´ ë°°ê²½/ì›ì¸ì„ ìœ ì¶”í•´ì¤˜.
# 4. ì‹¤ì§ˆì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ 2~3ê°€ì§€ ì œì‹œí•´ì¤˜.
# 5. ìƒë‹´ì ìŠ¤ìŠ¤ë¡œ ë³€í™”í•  ìˆ˜ ìˆë‹¤ëŠ” í¬ë§ì ì¸ ë©”ì‹œì§€ë¡œ ë§ˆë¬´ë¦¬í•´ì¤˜.

# ë§íˆ¬ëŠ” ìƒë‹´ìì™€ì˜ ì‹ ë¢° í˜•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ ,
# ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê°ì • ê³µê°ê³¼ ì§ˆë¬¸ì ì¤‘ì‹¬ì˜ ìƒë‹´ì— ì§‘ì¤‘í•´ì¤˜.

# ë°˜ë“œì‹œ "ì‚¬ìš°ë‹˜"ì´ ì•„ë‹ˆë¼ "ìƒë‹´ìë‹˜"ì´ë¼ê³  ë¶€ë¥´ê³ ,
# ë‹µë³€ì€ ê¸¸ì§€ ì•Šê²Œ 3~5 ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ìƒë‹´ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [3] FastAPI ì—”ë“œí¬ì¸íŠ¸ ===
# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")

#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 300,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     async with httpx.AsyncClient(timeout=60.0) as client:
#         res = await client.post(OLLAMA_URL, json=payload)
#         result = res.json()

#     return {
#         "ai_response": result.get("response", "").strip()
#     }

######### ë‹¨ê¸° ê¸°ì–µ ê¸°ëŠ¥ ì¶”ê°€
# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# chat_memory: dict[str, list[str]] = {}

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
#     context = "\n\n".join(retrieved_docs)

#     return f"""
# ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ì„ìƒì‹¬ë¦¬í•™ê³¼ ì •ì‹ ê±´ê°• ìƒë‹´í•™ì„ ì „ê³µí•œ ì „ë¬¸ê°€ë¡œì„œ, ê°ì •ì— ê¹Šì´ ê³µê°í•˜ë©´ì„œë„ ë¬¸ì œì˜ ë³¸ì§ˆì„ í•¨ê»˜ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•´.

# ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ë¬¸ì œë¡œ í˜ë“¤ì–´í•˜ê³  ìˆì–´.
# ìƒë‹´ìì˜ ë§ì—ì„œ ëŠê»´ì§€ëŠ” **ê°ì •ì˜ ì¢…ë¥˜ì™€ ê°•ë„**ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ê·¸ ê°ì •ì— ë§ëŠ” ë§íˆ¬ì™€ ì¡°ì–¸ ë°©ì‹ì„ íƒí•´ì¤˜.

# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì´ì–´ì§€ëŠ” íë¦„ìœ¼ë¡œ ìƒë‹´í•´ì¤˜:

# {history_text}

# ë‹µë³€ êµ¬ì„±ì€ ì•„ë˜ ìˆœì„œëŒ€ë¡œ í•´:
# 1. ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜.
# 2. ê·¸ ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•´ì¤˜.
# 3. ê°ì •ì´ ìƒê¸´ ë°°ê²½/ì›ì¸ì„ ìœ ì¶”í•´ì¤˜.
# 4. ì‹¤ì§ˆì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ 2~3ê°€ì§€ ì œì‹œí•´ì¤˜.
# 5. ìƒë‹´ì ìŠ¤ìŠ¤ë¡œ ë³€í™”í•  ìˆ˜ ìˆë‹¤ëŠ” í¬ë§ì ì¸ ë©”ì‹œì§€ë¡œ ë§ˆë¬´ë¦¬í•´ì¤˜.

# ë§íˆ¬ëŠ” ìƒë‹´ìì™€ì˜ ì‹ ë¢° í˜•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ ,
# ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê°ì • ê³µê°ê³¼ ì§ˆë¬¸ì ì¤‘ì‹¬ì˜ ìƒë‹´ì— ì§‘ì¤‘í•´ì¤˜.

# ë°˜ë“œì‹œ \"ì‚¬ìš°ë‹˜\"ì´ ì•„ë‹ˆë¼ \"ìƒë‹´ìë‹˜\"ì´ë¼ê³  ë¶€ë¥´ê³ ,
# ë‹µë³€ì€ ê¸¸ì§€ ì•Šê²Œ 3~5 ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ìƒë‹´ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory:
#         chat_memory[user_id] = []
#     chat_memory[user_id].append(f"ìƒë‹´ìë‹˜: {user_input}")
#     history_text = "\n".join(chat_memory[user_id][-5:])

#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 150,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#         # "stop": ["\n\nìƒë‹´ìë‹˜", "ê·¸ëŸ¼ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”", "</s>"]
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#     except Exception as e:
#         print("[âŒ ì˜ˆì™¸ ë°œìƒ]", str(e))
#         return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

#     ai_text = result.get("response")
#     if not ai_text:
#         ai_text = "ìƒë‹´ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

#     chat_memory[user_id].append(f"AI: {ai_text.strip()}")

#     return {
#         "ai_response": ai_text.strip()
#     }



######## ë‹¨ê¸°ê¸°ì–µ 2ë²ˆì§¸ ë²„ì „(í”„ë¡¬í”„íŠ¸ ë‚´ì— ê¸°ì–µ ìœ„ì¹˜ ë³€ê²½)
# from fastapi import FastAPI, Request
# from fastapi.middleware.cors import CORSMiddleware
# import httpx
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# OLLAMA_URL = "http://localhost:11434/api/generate"
# MODEL_NAME = "gemma3:12b-it-qat"

# retriever_model = SentenceTransformer("BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# chat_memory: dict[str, list[str]] = {}

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
#     context = "\n\n".join(retrieved_docs)

#     return f"""
# ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
# ì„ìƒì‹¬ë¦¬í•™ê³¼ ì •ì‹ ê±´ê°• ìƒë‹´í•™ì„ ì „ê³µí•œ ì „ë¬¸ê°€ë¡œì„œ, ê°ì •ì— ê¹Šì´ ê³µê°í•˜ë©´ì„œë„ ë¬¸ì œì˜ ë³¸ì§ˆì„ í•¨ê»˜ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•´.

# ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ë¬¸ì œë¡œ í˜ë“¤ì–´í•˜ê³  ìˆì–´.
# ìƒë‹´ìì˜ ë§ì—ì„œ ëŠê»´ì§€ëŠ” **ê°ì •ì˜ ì¢…ë¥˜ì™€ ê°•ë„**ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ê·¸ ê°ì •ì— ë§ëŠ” ë§íˆ¬ì™€ ì¡°ì–¸ ë°©ì‹ì„ íƒí•´ì¤˜.

# ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” íë¦„ì€ ì•„ë˜ì™€ ê°™ì•„. ë°˜ë“œì‹œ ì´ ë§¥ë½ì„ ê¸°ì–µí•˜ê³ , ì´ì–´ì„œ ë‹µë³€í•´ì¤˜:

# {history_text}

# ë‹µë³€ êµ¬ì„±ì€ ì•„ë˜ ìˆœì„œëŒ€ë¡œ í•´:
# 1. ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜.
# 2. ê·¸ ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•´ì¤˜.
# 3. ê°ì •ì´ ìƒê¸´ ë°°ê²½/ì›ì¸ì„ ìœ ì¶”í•´ì¤˜.
# 4. ì‹¤ì§ˆì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ 2~3ê°€ì§€ ì œì‹œí•´ì¤˜.
# 5. ìƒë‹´ì ìŠ¤ìŠ¤ë¡œ ë³€í™”í•  ìˆ˜ ìˆë‹¤ëŠ” í¬ë§ì ì¸ ë©”ì‹œì§€ë¡œ ë§ˆë¬´ë¦¬í•´ì¤˜.

# ë§íˆ¬ëŠ” ìƒë‹´ìì™€ì˜ ì‹ ë¢° í˜•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ ,
# ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê°ì • ê³µê°ê³¼ ì§ˆë¬¸ì ì¤‘ì‹¬ì˜ ìƒë‹´ì— ì§‘ì¤‘í•´ì¤˜.

# ë°˜ë“œì‹œ "ì‚¬ìš°ë‹˜"ì´ ì•„ë‹ˆë¼ "ìƒë‹´ìë‹˜"ì´ë¼ê³  ë¶€ë¥´ê³ ,
# ë‹µë³€ì€ ê¸¸ì§€ ì•Šê²Œ 3~5 ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ìƒë‹´ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# @app.post("/chat")
# async def chat(request: Request):
#     body = await request.json()
#     user_input = body.get("message", "")
#     user_id = body.get("user_id", "default")

#     if user_id not in chat_memory:
#         chat_memory[user_id] = []

#     chat_memory[user_id].append(f"ìƒë‹´ìë‹˜: {user_input}")
#     history_text = "\n".join(chat_memory[user_id][-6:])  # ìµœê·¼ 3ìŒ (ìƒë‹´ì+AI)

#     retrieved_docs = retrieve_relevant_chunks(user_input)
#     prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

#     payload = {
#         "model": MODEL_NAME,
#         "prompt": prompt,
#         "stream": False,
#         "num_predict": 150,
#         "top_k": 40,
#         "top_p": 0.9,
#         "temperature": 0.7
#     }

#     try:
#         async with httpx.AsyncClient(timeout=120.0) as client:
#             res = await client.post(OLLAMA_URL, json=payload)
#             result = res.json()
#     except Exception as e:
#         print("[âŒ ì˜ˆì™¸ ë°œìƒ]", str(e))
#         return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

#     ai_text = result.get("response")
#     if not ai_text:
#         ai_text = "ìƒë‹´ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

#     chat_memory[user_id].append(f"AI: {ai_text.strip()}")

#     return {
#         "ai_response": ai_text.strip()
#     }


### ë ˆí¬íŠ¸ ìš”ì•½ ê¸°ëŠ¥ ì¶”ê°€
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import httpx
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "gemma3:12b-it-qat"

retriever_model = SentenceTransformer("BAAI/bge-m3")
index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

chat_memory: dict[str, list[str]] = {}

def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
    query_text = f"ì§ˆë¬¸: {query}"
    query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
    faiss.normalize_L2(query_vec)
    _, I = index.search(query_vec, top_k)
    return [chunks[i] for i in I[0]]

def build_rag_prompt(query: str, retrieved_docs: list[str], history_text: str) -> str:
    context = "\n\n".join(retrieved_docs)

    return f"""
ë„ˆëŠ” ë”°ëœ»í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.
ì„ìƒì‹¬ë¦¬í•™ê³¼ ì •ì‹ ê±´ê°• ìƒë‹´í•™ì„ ì „ê³µí•œ ì „ë¬¸ê°€ë¡œì„œ, ê°ì •ì— ê¹Šì´ ê³µê°í•˜ë©´ì„œë„ ë¬¸ì œì˜ ë³¸ì§ˆì„ í•¨ê»˜ ì°¾ì•„ì£¼ëŠ” ì—­í• ì„ í•´.

ì´ë²ˆ ìƒë‹´ìëŠ” ìì¡´ê°, ë¶ˆì•ˆ, ìš°ìš¸ ê°™ì€ ë¬¸ì œë¡œ í˜ë“¤ì–´í•˜ê³  ìˆì–´.
ìƒë‹´ìì˜ ë§ì—ì„œ ëŠê»´ì§€ëŠ” **ê°ì •ì˜ ì¢…ë¥˜ì™€ ê°•ë„**ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ê·¸ ê°ì •ì— ë§ëŠ” ë§íˆ¬ì™€ ì¡°ì–¸ ë°©ì‹ì„ íƒí•´ì¤˜.

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” íë¦„ì€ ì•„ë˜ì™€ ê°™ì•„. ë°˜ë“œì‹œ ì´ ë§¥ë½ì„ ê¸°ì–µí•˜ê³ , ì´ì–´ì„œ ë‹µë³€í•´ì¤˜:

{history_text}

ë‹µë³€ êµ¬ì„±ì€ ì•„ë˜ ìˆœì„œëŒ€ë¡œ í•´:
1. ìƒë‹´ìì˜ ê°ì •ì„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¶”ì¸¡í•´ì¤˜.
2. ê·¸ ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•´ì¤˜.
3. ê°ì •ì´ ìƒê¸´ ë°°ê²½/ì›ì¸ì„ ìœ ì¶”í•´ì¤˜.
4. ì‹¤ì§ˆì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ 2~3ê°€ì§€ ì œì‹œí•´ì¤˜.
5. ìƒë‹´ì ìŠ¤ìŠ¤ë¡œ ë³€í™”í•  ìˆ˜ ìˆë‹¤ëŠ” í¬ë§ì ì¸ ë©”ì‹œì§€ë¡œ ë§ˆë¬´ë¦¬í•´ì¤˜.

ë§íˆ¬ëŠ” ìƒë‹´ìì™€ì˜ ì‹ ë¢° í˜•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ ,
ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê°ì • ê³µê°ê³¼ ì§ˆë¬¸ì ì¤‘ì‹¬ì˜ ìƒë‹´ì— ì§‘ì¤‘í•´ì¤˜.

ë°˜ë“œì‹œ \"ì‚¬ìš°ë‹˜\"ì´ ì•„ë‹ˆë¼ \"ìƒë‹´ìë‹˜\"ì´ë¼ê³  ë¶€ë¥´ê³ ,
ë‹µë³€ì€ ê¸¸ì§€ ì•Šê²Œ 3~5 ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

[ì°¸ê³ ìë£Œ]
{context}

[ìƒë‹´ì ì§ˆë¬¸]
{query}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
"""

@app.post("/chat")
async def chat(request: Request):
    body = await request.json()
    user_input = body.get("message", "")
    user_id = body.get("user_id", "default")

    if user_id not in chat_memory:
        chat_memory[user_id] = []

    chat_memory[user_id].append(f"ìƒë‹´ìë‹˜: {user_input}")
    history_text = "\n".join(chat_memory[user_id][-6:])

    retrieved_docs = retrieve_relevant_chunks(user_input)
    prompt = build_rag_prompt(user_input, retrieved_docs, history_text)

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "num_predict": 150,
        "top_k": 40,
        "top_p": 0.9,
        "temperature": 0.7
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            res = await client.post(OLLAMA_URL, json=payload)
            result = res.json()
    except Exception as e:
        print("[âŒ ì˜ˆì™¸ ë°œìƒ]", str(e))
        return {"ai_response": "ëª¨ë¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    ai_text = result.get("response")
    if not ai_text:
        ai_text = "ìƒë‹´ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    chat_memory[user_id].append(f"AI: {ai_text.strip()}")

    return {
        "ai_response": ai_text.strip()
    }

# === ê°ì •ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ ===
def build_summary_prompt(history_text: str) -> str:
    return f"""
ë„ˆëŠ” ì‹¬ë¦¬ìƒë‹´ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ìš”ì•½í•˜ê³ ,
ê·¸ ê°ì •ì˜ ì›ì¸ê³¼ íŠ¹ì„±ì„ ì •ë¦¬í•œ ì§§ì€ ë ˆí¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ì—­í• ì„ í•´.

ì•„ë˜ëŠ” ìƒë‹´ìì™€ AIì˜ ì „ì²´ ëŒ€í™” ë‚´ìš©ì´ì•¼:

{history_text}

ì´ ìƒë‹´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì„ í¬í•¨í•œ ì§§ì€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜:
1. í˜„ì¬ ê°ì • ìƒíƒœ ìš”ì•½ (ì˜ˆ: ë¶ˆì•ˆ, ìš°ìš¸, ë¶„ë…¸ ë“± ë³µìˆ˜ ê°€ëŠ¥)
2. ê°ì •ì´ ìœ ë°œëœ ì£¼ìš” ì›ì¸
3. ìƒë‹´ìê°€ íŠ¹íˆ í˜ë“¤ì–´í–ˆë˜ ë¶€ë¶„
4. ìƒë‹´ìê°€ ìì£¼ ë°˜ë³µí•´ì„œ í‘œí˜„í•œ ê°ì •ì´ë‚˜ ìƒê°
5. ì¢…í•© í‰ê°€ ë° íšŒë³µì„ ìœ„í•œ ì¡°ì–¸

ì‘ë‹µì€ ë¬¸ë‹¨ í˜•íƒœë¡œ ì‘ì„±í•˜ê³ , ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì •ëˆí•´ì„œ ì¶œë ¥í•´ì¤˜.
"""

@app.post("/summary")
async def summarize(request: Request):
    body = await request.json()
    user_id = body.get("user_id", "default")

    if user_id not in chat_memory or not chat_memory[user_id]:
        return {"summary": "ìš”ì•½í•  ìƒë‹´ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}

    history_text = "\n".join(chat_memory[user_id])
    prompt = build_summary_prompt(history_text)

    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False,
        "num_predict": 300,
        "top_k": 40,
        "top_p": 0.9,
        "temperature": 0.7
    }

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            res = await client.post(OLLAMA_URL, json=payload)
            result = res.json()
    except Exception as e:
        print("[âŒ ìš”ì•½ ì˜¤ë¥˜ ë°œìƒ]", str(e))
        return {"summary": "ë ˆí¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    summary = result.get("response", "").strip()
    return {"summary": summary}
