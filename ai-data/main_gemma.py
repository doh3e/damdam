import os
from typing import List, Optional

from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from pydantic import BaseModel
import torch
import faiss
import json
import numpy as np

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===

load_dotenv()

# Huggingface Gemma 3 ë¡œì»¬ LLM ë¡œë”©
gemma_model_name = "google/gemma-2b-it"  # ë” ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½
print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")
tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
model = AutoModelForCausalLM.from_pretrained(
    gemma_model_name,
    device_map="auto",
    torch_dtype=torch.float32  # CPU í™˜ê²½ì— ì í•©í•˜ë„ë¡ float32 ì‚¬ìš©
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1000,
    temperature=0.5,
    top_p=0.9,
    do_sample=True
)

gemma_llm = HuggingFacePipeline(pipeline=pipe)
print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
index_to_docstore_id = {i: str(i) for i in range(len(docs))}
docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

vectorstore = FAISS(
    embedding_model.embed_query,
    index,
    docstore,
    index_to_docstore_id
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# === [3] FastAPI ì•± ì •ì˜ ===

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===

class ChatMessage(BaseModel):
    role: str
    content: str

class AssistantRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None

class ChatRequest(BaseModel):
    messages: List[ChatMessage]

class MessageRequest(BaseModel):
    message: str

# === [5] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ===

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.

ì•„ë˜ëŠ” ì‹¤ì œë¡œ ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ìš©ìë“¤ê³¼ì˜ ìƒë‹´ ì‚¬ë¡€ë“¤ì´ì•¼. ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ, ì§€ê¸ˆ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” **ìƒˆë¡œìš´ ì¡°ì–¸**ì„ **ë„ˆì˜ ë§ë¡œ ì§ì ‘** ìƒì„±í•´ì„œ ì „ë‹¬í•´ ì¤˜.

ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
1. ì°¸ê³ ìë£ŒëŠ” ì ˆëŒ€ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ ë§ˆ. ìš”ì•½í•˜ê±°ë‚˜ ë„ˆë§Œì˜ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì¨.
2. ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ì¶”ë¡ í•˜ê³  ê·¸ì— ê³µê°í•˜ëŠ” ë§ì„ ë¨¼ì € í•´ ì¤˜.
3. ì´í›„ ì‹¤ì§ˆì ì¸ ì¡°ì–¸ì´ë‚˜ ìœ„ë¡œ, ì œì•ˆì´ ì´ì–´ì ¸ì•¼ í•´.
4. í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•´ ë§íˆ¬ë¥¼ ë§ì¶°ì¤˜.

ğŸ“š ìƒë‹´ ì˜ˆì‹œ (few-shot)
ì‚¬ìš©ì: ìš”ì¦˜ ë„ˆë¬´ ë¬´ê¸°ë ¥í•˜ê³  ì•„ë¬´ ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”.
ìƒë‹´ì‚¬: ê·¸ëŸ´ ìˆ˜ ìˆì–´ìš”. ëª¸ì´ ë„ˆë¬´ ì§€ì³¤ê±°ë‚˜, ë§ˆìŒì´ í…… ë¹ˆ ëŠë‚Œì´ ë“¤ ë• ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì„ ìˆ˜ ìˆì–´ìš”. ìš°ì„  ê·¸ëŸ° ë§ˆìŒì„ ì–µì§€ë¡œ ë°”ê¾¸ë ¤ í•˜ì§€ ì•Šì•„ë„ ê´œì°®ì•„ìš”. ì ê¹ ì‰¬ëŠ” ê²ƒë„ ë°©ë²•ì´ì—ìš”.

ì‚¬ìš©ì: ì‚¬ëŒë“¤ì´ ë‹¤ ì €ë¥¼ ì‹«ì–´í•˜ëŠ” ê²ƒ ê°™ì•„ìš”.
ìƒë‹´ì‚¬: ê·¸ëŸ° ìƒê°ì´ ë“¤ë©´ ë§ì´ ì™¸ë¡­ê³  ë¶ˆì•ˆí•˜ì…¨ì„ ê²ƒ ê°™ì•„ìš”. ê·¸ëŸ°ë° ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì€ íƒ€ì¸ì˜ ìƒê°ë³´ë‹¤ ìê¸° ì¼ì— ë” ì§‘ì¤‘í•´ìš”. í˜¹ì‹œ ë„ˆë¬´ í˜¼ìì„œ ê³ ë¯¼í•˜ê³  ìˆì§„ ì•Šë‚˜ìš”?

[ì°¸ê³ ìë£Œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]

    """
)

# === [6] ìƒë‹´ ì‘ë‹µ API ===

@app.post("/chat")
async def chat_endpoint(req: MessageRequest):
    print("ğŸ“© ë°›ì€ ìš”ì²­:", req)
    print("ğŸ“¨ message ë‚´ìš©:", repr(req.message))

    if not req.message or not req.message.strip():
        return {
            "reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™",
            "sources": []
        }

    print("ğŸ§± QA ì²´ì¸ ìƒì„± ì‹œì‘")
    qa = RetrievalQA.from_chain_type(
        llm=gemma_llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": custom_prompt}
    )

    print("ğŸš€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì‹œì‘")
    result = qa.invoke(req.message)
    print("âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")

    source_docs = result['source_documents']
    sources = [{"content": doc.page_content, "metadata": doc.metadata} for doc in source_docs]

    for i, source in enumerate(sources):
        print(f"ğŸ“š Source {i + 1}: {source['content']}")
        print(f"ğŸ—‚ Metadata: {source['metadata']}")

    return {
        "reply": result['result'],
        "sources": sources
    }

# === [7] í—¬ìŠ¤ ì²´í¬ ===

@app.get("/health")
@app.get("/")
async def health_check():
    return {"status": "ok"}

# === [8] ì‹¤í–‰ ===

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
