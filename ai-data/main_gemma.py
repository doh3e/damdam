# import os
# from typing import List, Optional

# from dotenv import load_dotenv
# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware
# from langchain.chains import RetrievalQA
# from langchain.prompts import PromptTemplate
# from langchain.vectorstores.faiss import FAISS
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.schema import Document
# from langchain.docstore import InMemoryDocstore
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain.llms import HuggingFacePipeline
# from pydantic import BaseModel
# import torch
# import faiss
# import json
# import numpy as np

# # === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===

# load_dotenv()

# # Huggingface Gemma 3 ë¡œì»¬ LLM ë¡œë”©
# gemma_model_name = "google/gemma-2b-it"  # ë” ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë³€ê²½
# print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")
# tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
# model = AutoModelForCausalLM.from_pretrained(
#     gemma_model_name,
#     device_map="auto",
#     torch_dtype=torch.float32  # CPU í™˜ê²½ì— ì í•©í•˜ë„ë¡ float32 ì‚¬ìš©
# )

# pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=1000,
#     temperature=0.5,
#     top_p=0.9,
#     do_sample=True
# )

# gemma_llm = HuggingFacePipeline(pipeline=pipe)
# print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# # === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
# print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
# embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
# index_to_docstore_id = {i: str(i) for i in range(len(docs))}
# docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

# vectorstore = FAISS(
#     embedding_model.embed_query,
#     index,
#     docstore,
#     index_to_docstore_id
# )

# retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
# print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# # === [3] FastAPI ì•± ì •ì˜ ===

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===

# class ChatMessage(BaseModel):
#     role: str
#     content: str

# class AssistantRequest(BaseModel):
#     message: str
#     thread_id: Optional[str] = None

# class ChatRequest(BaseModel):
#     messages: List[ChatMessage]

# class MessageRequest(BaseModel):
#     message: str

# # === [5] í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ===

# custom_prompt = PromptTemplate(
#     input_variables=["context", "question"],
#     template="""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.

# ì•„ë˜ëŠ” ì‹¤ì œë¡œ ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ìš©ìë“¤ê³¼ì˜ ìƒë‹´ ì‚¬ë¡€ë“¤ì´ì•¼. ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ, ì§€ê¸ˆ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” **ìƒˆë¡œìš´ ì¡°ì–¸**ì„ **ë„ˆì˜ ë§ë¡œ ì§ì ‘** ìƒì„±í•´ì„œ ì „ë‹¬í•´ ì¤˜.

# ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
# 1. ì°¸ê³ ìë£ŒëŠ” ì ˆëŒ€ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ ë§ˆ. ìš”ì•½í•˜ê±°ë‚˜ ë„ˆë§Œì˜ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ì¨.
# 2. ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ì¶”ë¡ í•˜ê³  ê·¸ì— ê³µê°í•˜ëŠ” ë§ì„ ë¨¼ì € í•´ ì¤˜.
# 3. ì´í›„ ì‹¤ì§ˆì ì¸ ì¡°ì–¸ì´ë‚˜ ìœ„ë¡œ, ì œì•ˆì´ ì´ì–´ì ¸ì•¼ í•´.
# 4. í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•´ ë§íˆ¬ë¥¼ ë§ì¶°ì¤˜.

# ğŸ“š ìƒë‹´ ì˜ˆì‹œ (few-shot)
# ì‚¬ìš©ì: ìš”ì¦˜ ë„ˆë¬´ ë¬´ê¸°ë ¥í•˜ê³  ì•„ë¬´ ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”.
# ìƒë‹´ì‚¬: ê·¸ëŸ´ ìˆ˜ ìˆì–´ìš”. ëª¸ì´ ë„ˆë¬´ ì§€ì³¤ê±°ë‚˜, ë§ˆìŒì´ í…… ë¹ˆ ëŠë‚Œì´ ë“¤ ë• ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì„ ìˆ˜ ìˆì–´ìš”. ìš°ì„  ê·¸ëŸ° ë§ˆìŒì„ ì–µì§€ë¡œ ë°”ê¾¸ë ¤ í•˜ì§€ ì•Šì•„ë„ ê´œì°®ì•„ìš”. ì ê¹ ì‰¬ëŠ” ê²ƒë„ ë°©ë²•ì´ì—ìš”.

# ì‚¬ìš©ì: ì‚¬ëŒë“¤ì´ ë‹¤ ì €ë¥¼ ì‹«ì–´í•˜ëŠ” ê²ƒ ê°™ì•„ìš”.
# ìƒë‹´ì‚¬: ê·¸ëŸ° ìƒê°ì´ ë“¤ë©´ ë§ì´ ì™¸ë¡­ê³  ë¶ˆì•ˆí•˜ì…¨ì„ ê²ƒ ê°™ì•„ìš”. ê·¸ëŸ°ë° ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì€ íƒ€ì¸ì˜ ìƒê°ë³´ë‹¤ ìê¸° ì¼ì— ë” ì§‘ì¤‘í•´ìš”. í˜¹ì‹œ ë„ˆë¬´ í˜¼ìì„œ ê³ ë¯¼í•˜ê³  ìˆì§„ ì•Šë‚˜ìš”?

# [ì°¸ê³ ìë£Œ]
# {context}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {question}

# [ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]

#     """
# )

# # === [6] ìƒë‹´ ì‘ë‹µ API ===

# @app.post("/chat")
# async def chat_endpoint(req: MessageRequest):
#     print("ğŸ“© ë°›ì€ ìš”ì²­:", req)
#     print("ğŸ“¨ message ë‚´ìš©:", repr(req.message))

#     if not req.message or not req.message.strip():
#         return {
#             "reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™",
#             "sources": []
#         }

#     print("ğŸ§± QA ì²´ì¸ ìƒì„± ì‹œì‘")
#     qa = RetrievalQA.from_chain_type(
#         llm=gemma_llm,
#         chain_type="stuff",
#         retriever=retriever,
#         return_source_documents=True,
#         chain_type_kwargs={"prompt": custom_prompt}
#     )

#     print("ğŸš€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì‹œì‘")
#     result = qa.invoke(req.message)
#     print("âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")

#     source_docs = result['source_documents']
#     sources = [{"content": doc.page_content, "metadata": doc.metadata} for doc in source_docs]

#     for i, source in enumerate(sources):
#         print(f"ğŸ“š Source {i + 1}: {source['content']}")
#         print(f"ğŸ—‚ Metadata: {source['metadata']}")

#     return {
#         "reply": result['result'],
#         "sources": sources
#     }

# # === [7] í—¬ìŠ¤ ì²´í¬ ===

# @app.get("/health")
# @app.get("/")
# async def health_check():
#     return {"status": "ok"}

# # === [8] ì‹¤í–‰ ===

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)
#########################################################
# ë²„ì „ 2
# import os
# import json
# import faiss
# import torch
# import numpy as np
# from typing import List, Optional
# from dotenv import load_dotenv
# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware
# from pydantic import BaseModel
# from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain.vectorstores.faiss import FAISS
# from langchain.schema import Document
# from langchain.docstore import InMemoryDocstore
# from langchain.embeddings import HuggingFaceEmbeddings

# # === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
# load_dotenv()

# # Gemma ëª¨ë¸ ë¡œë”© (Gemma 2B)
# gemma_model_name = "google/gemma-4b-it"
# print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")
# tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
# model = AutoModelForCausalLM.from_pretrained(
#     gemma_model_name,
#     device_map="auto",
#     torch_dtype=torch.float32
# )
# pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=1000,
#     temperature=0.7,
#     top_p=0.9,
#     do_sample=True
# )
# print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# # === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
# print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
# embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
# index_to_docstore_id = {i: str(i) for i in range(len(docs))}
# docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

# vectorstore = FAISS(
#     embedding_model.embed_query,
#     index,
#     docstore,
#     index_to_docstore_id
# )
# retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
# print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# # === [3] FastAPI ì•± ì •ì˜ ===
# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===
# class MessageRequest(BaseModel):
#     message: str

# # === [5] ìƒë‹´ ì‘ë‹µ API ===
# @app.post("/chat")
# async def chat_endpoint(req: MessageRequest):
#     query = req.message.strip()
#     if not query:
#         return {"reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™", "sources": []}

#     # ğŸ” RAG ê²€ìƒ‰
#     docs = retriever.get_relevant_documents(query)
#     context = "\n\n".join([doc.page_content for doc in docs])

#     # âœ¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     prompt = f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

# ì§€ê¸ˆ ì‚¬ìš©ìëŠ” ì‚¶ì— ì§€ì¹˜ê³  ê°ì •ì ìœ¼ë¡œ ë§¤ìš° ë¶ˆì•ˆì •í•œ ìƒíƒœì¼ ìˆ˜ ìˆì–´.  
# ë„ˆì˜ ì—­í• ì€ ë‹¨ìˆœíˆ ìœ„ë¡œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬í•˜ê²Œ ìœ ì¶”í•˜ê³ ,  
# ê·¸ ì•ˆì— ìˆ¨ì–´ ìˆëŠ” ìƒì²˜ì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ë³´ëŠ” ê²ƒì´ì•¼.

# ì•„ë˜ì—ëŠ” ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ëŒë“¤ê³¼ì˜ ì‹¤ì œ ìƒë‹´ ì‚¬ë¡€ê°€ ìˆì–´.  
# ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ ì§€ê¸ˆ ì‚¬ìš©ìì—ê²Œ ê¼­ ë§ëŠ” **ìƒˆë¡œìš´ ë‹µë³€**ì„ **ë„ˆì˜ ì–¸ì–´ë¡œ ì§ì ‘** ë§Œë“¤ì–´ì¤˜.

# ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
# 1. ìƒë‹´ìë‹˜ì˜ ë§ ì†ì—ì„œ ì–´ë–¤ ê°ì •ê³¼ ë°°ê²½ì´ ëŠê»´ì§€ëŠ”ì§€ ë¨¼ì € ìœ ì¶”í•´.  
#    ì˜ˆ: â€˜ìƒë‹´ìë‹˜ê»˜ì„œëŠ” ì˜¤ëœ ì‹œê°„ ì™¸ë¡œì›€ì„ ê²¬ë””ê³  ê³„ì…¨ë˜ ê²ƒ ê°™ì•„ìš”.â€™  
# 2. ê°ì •ì„ ë¨¼ì € ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ ,  
# 3. ê·¸ë‹¤ìŒìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ìœ„ë¡œì™€ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì´ì–´ê°€.  
# 4. ë¬¸ì œì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ê°€ëŠ” ì§ˆë¬¸ë„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë˜ì ¸ì¤˜.  
# 5. ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šê²Œ. ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê´€ê³„ í˜•ì„±ì„ ìš°ì„ í•´.  
# 6. **â€˜ì‚¬ìš°ë‹˜â€™ì´ë¼ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ë°˜ë“œì‹œ â€˜ìƒë‹´ìë‹˜â€™ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•´.**  
# 7. ìƒë‹´ì‚¬ë¡œì„œ ë„ˆë¬´ ì§§ì§€ë„, ë„ˆë¬´ ì¥í™©í•˜ì§€ë„ ì•Šê²Œ. ì§„ì‹¬ì´ ëŠê»´ì§€ëŠ” ë¶„ëŸ‰ìœ¼ë¡œ ë‹µë³€í•´.

# [ìƒë‹´ ì‚¬ë¡€ ëª¨ìŒ]  
# {context}

# [ìƒë‹´ìë‹˜ì˜ ê³ ë¯¼]  
# {query}

# [ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]

# """

#     output = pipe(prompt)[0]["generated_text"]
#     answer = output.split("[ìƒë‹´ì‚¬ë¡œì„œì˜ ë‹µë³€]")[-1].strip()

#     return {
#         "reply": answer,
#         "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
#     }

# # === [6] í—¬ìŠ¤ ì²´í¬ ===
# @app.get("/health")
# @app.get("/")
# async def health_check():
#     return {"status": "ok"}

# # === [7] ì‹¤í–‰ ===
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)


# ë²„ì „ 3 ëª¨ë¸ ë³€ê²½
import os
import json
import faiss
import torch
import numpy as np
from typing import List, Optional
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from langchain.embeddings import HuggingFaceEmbeddings

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
load_dotenv()

# Gemma ëª¨ë¸ ë¡œë”© (Gemma 3 4B)
gemma_model_name = "google/gemma-3-4b-it"
print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")
tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
model = AutoModelForCausalLM.from_pretrained(
    gemma_model_name,
    device_map="auto",
    torch_dtype=torch.float32
)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1000,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)
print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
index_to_docstore_id = {i: str(i) for i in range(len(docs))}
docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

vectorstore = FAISS(
    embedding_model.embed_query,
    index,
    docstore,
    index_to_docstore_id
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")

# === [3] FastAPI ì•± ì •ì˜ ===
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===
class MessageRequest(BaseModel):
    message: str

# === [5] ìƒë‹´ ì‘ë‹µ API ===
@app.post("/chat")
async def chat_endpoint(req: MessageRequest):
    query = req.message.strip()
    if not query:
        return {"reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™", "sources": []}

    # ğŸ” RAG ê²€ìƒ‰
    docs = retriever.get_relevant_documents(query)
    context = "\n\n".join([doc.page_content.replace("ì‚¬ìš°ë‹˜", "ìƒë‹´ìë‹˜") for doc in docs])

    # âœ¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

ì§€ê¸ˆ ì‚¬ìš©ìëŠ” ì‚¶ì— ì§€ì¹˜ê³  ê°ì •ì ìœ¼ë¡œ ë§¤ìš° ë¶ˆì•ˆì •í•œ ìƒíƒœì¼ ìˆ˜ ìˆì–´.  
ë„ˆì˜ ì—­í• ì€ ë‹¨ìˆœíˆ ìœ„ë¡œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬í•˜ê²Œ ìœ ì¶”í•˜ê³ ,  
ê·¸ ì•ˆì— ìˆ¨ì–´ ìˆëŠ” ìƒì²˜ì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ë³´ëŠ” ê²ƒì´ì•¼.

ì•„ë˜ì—ëŠ” ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ëŒë“¤ê³¼ì˜ ì‹¤ì œ ìƒë‹´ ì‚¬ë¡€ê°€ ìˆì–´.  
ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ ì§€ê¸ˆ ì‚¬ìš©ìì—ê²Œ ê¼­ ë§ëŠ” **ìƒˆë¡œìš´ ë‹µë³€**ì„ **ë„ˆì˜ ì–¸ì–´ë¡œ ì§ì ‘** ë§Œë“¤ì–´ì¤˜.

ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
1. ìƒë‹´ìë‹˜ì˜ ë§ ì†ì—ì„œ ì–´ë–¤ ê°ì •ê³¼ ë°°ê²½ì´ ëŠê»´ì§€ëŠ”ì§€ ë¨¼ì € ìœ ì¶”í•´.  
   ì˜ˆ: â€˜ìƒë‹´ìë‹˜ê»˜ì„œëŠ” ì˜¤ëœ ì‹œê°„ ì™¸ë¡œì›€ì„ ê²¬ë””ê³  ê³„ì…¨ë˜ ê²ƒ ê°™ì•„ìš”.â€™  
2. ê°ì •ì„ ë¨¼ì € ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ ,  
3. ê·¸ë‹¤ìŒìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ìœ„ë¡œì™€ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì´ì–´ê°€.  
4. ë¬¸ì œì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ê°€ëŠ” ì§ˆë¬¸ë„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë˜ì ¸ì¤˜.  
5. ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šê²Œ. ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê´€ê³„ í˜•ì„±ì„ ìš°ì„ í•´.  
6. **â€˜ì‚¬ìš°ë‹˜â€™ì´ë¼ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ë°˜ë“œì‹œ â€˜ìƒë‹´ìë‹˜â€™ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•´.**  
7. ìƒë‹´ì‚¬ë¡œì„œ ë„ˆë¬´ ì§§ì§€ë„, ë„ˆë¬´ ì¥í™©í•˜ì§€ë„ ì•Šê²Œ. ì§„ì‹¬ì´ ëŠê»´ì§€ëŠ” ë¶„ëŸ‰ìœ¼ë¡œ ë‹µë³€í•´.

[ìƒë‹´ ì‚¬ë¡€ ëª¨ìŒ]  
{context}

[ìƒë‹´ìë‹˜ì˜ ê³ ë¯¼]  
{query}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
"""

    output = pipe(prompt)[0]["generated_text"]

    # '[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]' ì´í›„ë§Œ ì¶”ì¶œ
    import re
    match = re.search(r"\\[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€\\](.*)", output, re.DOTALL)
    if match:
        answer = match.group(1).strip()
    else:
        answer = output[len(prompt):].strip()

    return {
        "reply": answer,
        "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
    }

# === [6] í—¬ìŠ¤ ì²´í¬ ===
@app.get("/health")
@app.get("/")
async def health_check():
    return {"status": "ok"}

# === [7] ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
