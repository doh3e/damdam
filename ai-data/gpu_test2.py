import os
import json
import faiss
import torch
import numpy as np
from typing import List
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GenerationConfig
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from langchain.embeddings import HuggingFaceEmbeddings
import re
from collections import defaultdict

# === [0] ì„¸ì…˜ ê¸°ì–µ ===
memory_store = defaultdict(list)

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
load_dotenv()

gemma_model_name = "google/gemma-3-4b-it"
print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")

# ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
try:
    tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
    
    # âš ï¸ ì¤‘ìš”: CUDA ì»´í“¨íŒ… ëŠ¥ë ¥ í™•ì¸
    if torch.cuda.is_available():
        device_capability = torch.cuda.get_device_capability()
        print(f"CUDA ì»´í“¨íŒ… ëŠ¥ë ¥: {device_capability}")
        
        # í…ì„œì½”ì–´(Tensor Cores) ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸ (Ampere ì•„í‚¤í…ì²˜ ì´ìƒ)
        has_tensor_cores = device_capability[0] >= 8
        
        # ìµœì†Œí•œì˜ ì•ˆì „í•œ dtype ì„ íƒ
        if has_tensor_cores:
            print("âœ… í…ì„œì½”ì–´ ì§€ì› GPU - torch.bfloat16 ì‚¬ìš©")
            dtype = torch.bfloat16
        else:
            print("âš ï¸ í…ì„œì½”ì–´ ë¯¸ì§€ì› GPU - torch.float16 ì‚¬ìš©")
            dtype = torch.float16
    else:
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        dtype = torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        gemma_model_name,
        device_map="auto",
        torch_dtype=dtype
    )
    
    # ===== í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: CUDA ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•œ generate í•¨ìˆ˜ =====
    def text_generate(prompt, max_new_tokens=1000):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ëª…ì‹œì  ìƒì„± ì„¤ì • - multinomial ìƒ˜í”Œë§ ì˜¤ë¥˜ í•´ê²°
        generation_config = GenerationConfig(
            max_new_tokens=max_new_tokens,
            do_sample=False,  # greedy decoding ì‚¬ìš©
            temperature=1.0,  # ëª…ì‹œì ìœ¼ë¡œ ê°’ì„ ì„¤ì •í•´ë„ do_sample=Falseì¼ ë•ŒëŠ” ì‚¬ìš© ì•ˆ ë¨
            num_beams=1,      # beam search ë¹„í™œì„±í™”
            top_p=1.0,        # ëª…ì‹œì  ì„¤ì •
            top_k=50,         # ëª…ì‹œì  ì„¤ì •
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        # CUDA_LAUNCH_BLOCKING=1 íš¨ê³¼ë¥¼ ë‚´ê¸° ìœ„í•œ ë™ê¸°í™”
        torch.cuda.synchronize()
        
        with torch.no_grad():
            try:
                # ëª¨ë¸ì˜ ê¸°ë³¸ generation_config ê°’ ëª…ì‹œì ìœ¼ë¡œ ì¬ì •ì˜
                old_defaults = {}
                for key, value in generation_config.__dict__.items():
                    if hasattr(model.generation_config, key):
                        old_defaults[key] = getattr(model.generation_config, key)
                        setattr(model.generation_config, key, value)
                
                outputs = model.generate(
                    **inputs,
                    generation_config=generation_config
                )
                
                # ì…ë ¥ í† í° ìˆ˜ë¥¼ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
                result = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                
                # ì›ë˜ ìƒì„± êµ¬ì„± ë³µì›
                for key, value in old_defaults.items():
                    setattr(model.generation_config, key, value)
                
                return result
            except Exception as e:
                print(f"ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ ì „ëµ: íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì‹œë„
                try:
                    print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹œë„...")
                    pipe = pipeline(
                        "text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,
                        temperature=None
                    )
                    result = pipe(prompt)[0]['generated_text']
                    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
                    if result.startswith(prompt):
                        result = result[len(prompt):]
                    return result
                except Exception as e2:
                    print(f"íŒŒì´í”„ë¼ì¸ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: CPUë¡œ ë–¨ì–´ëœ¨ë ¤ì„œ ì‹¤í–‰
                    try:
                        print("CPUë¡œ fallback...")
                        # ì¼ì‹œì ìœ¼ë¡œ CPUë¡œ ëª¨ë¸ ì´ë™
                        current_device = next(model.parameters()).device
                        model_cpu = model.to("cpu")
                        inputs_cpu = {k: v.to("cpu") for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = model_cpu.generate(
                                **inputs_cpu,
                                max_new_tokens=max_new_tokens,
                                do_sample=False,
                                temperature=None
                            )
                        
                        # ë‹¤ì‹œ ì›ë˜ ì¥ì¹˜ë¡œ ë³µì›
                        model.to(current_device)
                        
                        result = tokenizer.decode(outputs[0][inputs_cpu['input_ids'].shape[1]:], skip_special_tokens=True)
                        return result
                    except Exception as e3:
                        print(f"CPU fallbackë„ ì‹¤íŒ¨: {e3}")
                        return "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
    print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ê²¬ê³ í•œ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©)")
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
    raise

# === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
try:
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    index = faiss.read_index("qa_index_bge_m3.faiss")
    with open("qa_chunks.json", "r", encoding="utf-8") as f:
        chunks = json.load(f)
    docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
    index_to_docstore_id = {i: str(i) for i in range(len(docs))}
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})
    vectorstore = FAISS(
        embedding_model.embed_query,
        index,
        docstore,
        index_to_docstore_id
    )
    # LangChain deprecation ê²½ê³  ìˆ˜ì •
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
    retriever = None

# === [3] FastAPI ì•± ì •ì˜ ===
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage] = []
    thread_id: str = "default"

# === [5] ìƒë‹´ ì‘ë‹µ API ===
@app.post("/chat")
async def chat_endpoint(request: Request):
    raw = await request.json()

    if "message" in raw:
        thread_id = raw.get("thread_id", "default")
        new_message = ChatMessage(role="user", content=raw["message"])
        memory_store[thread_id].append(new_message)
        messages = memory_store[thread_id]
    elif "messages" in raw:
        chat_req = ChatRequest(**raw)
        thread_id = chat_req.thread_id
        memory_store[thread_id].extend(chat_req.messages)
        messages = memory_store[thread_id]
    else:
        return {"reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™", "sources": []}

    current_question = messages[-1].content.strip()
    history = messages[:-1]

    # "ìì‚´" í‚¤ì›Œë“œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
    docs = []
    context = ""
    
    # ğŸ” RAG ê²€ìƒ‰ - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ ë° deprecation ê²½ê³  ìˆ˜ì •
    if retriever is not None:
        try:
            # ìƒˆë¡œìš´ invoke ë©”ì„œë“œ ì‚¬ìš© (LangChain ì—…ë°ì´íŠ¸ ë°˜ì˜)
            if hasattr(retriever, "invoke"):
                docs = retriever.invoke(current_question)
            else:
                # ì´ì „ ë©”ì„œë“œëŠ” ê²½ê³ ì™€ í•¨ê»˜ ê³„ì† ì‘ë™
                docs = retriever.get_relevant_documents(current_question)
                
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œí•œ
            if len(docs) > 3:
                docs = docs[:3]
            context = "\n\n".join([doc.page_content.replace("ì‚¬ìš°ë‹˜", "ìƒë‹´ìë‹˜") for doc in docs])
        except Exception as e:
            print(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            docs = []

    # ğŸ§  ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± - ìµœê·¼ ê¸°ë¡ë§Œ ìœ ì§€
    history_text = ""
    # ìµœê·¼ ëŒ€í™” ê¸°ë¡ë§Œ ìœ ì§€ (ì˜ˆ: ìµœê·¼ 3ê°œ)
    recent_history = history[-3:] if len(history) > 3 else history
    for msg in recent_history:
        role = "ìƒë‹´ìë‹˜" if msg.role == "user" else "AI"
        history_text += f"{role}: {msg.content}\n"

    # âœ¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
1. ìƒë‹´ìë‹˜ì˜ ê°ì •ì„ ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³  ìœ„ë¡œí•˜ê¸°
2. ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šì€ ë§íˆ¬ ì‚¬ìš©í•˜ê¸°
3. 'ì‚¬ìš°ë‹˜'ì´ë¼ëŠ” í‘œí˜„ì€ ì“°ì§€ ë§ê³  'ìƒë‹´ìë‹˜'ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•˜ê¸°
4. ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì œê³µí•˜ê¸°

[ì´ì „ ëŒ€í™”]  
{history_text}

[ìƒë‹´ ì°¸ê³  ìë£Œ]  
{context}

[ìƒë‹´ìë‹˜ì˜ í˜„ì¬ ê³ ë¯¼]  
{current_question}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
"""

    # âœ… ì‘ë‹µ ìƒì„± (ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©)
    try:
        answer = text_generate(prompt)
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ìƒë‹´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    # ìì‚´ ì–¸ê¸‰ì´ ìˆì„ ê²½ìš° ìœ„ê¸° ëŒ€ì‘ ë©”ì‹œì§€ ì¶”ê°€
    crisis_keywords = ["ìì‚´", "ì£½ê³ ", "ì£½ì„", "ëª©ìˆ¨", "ëª©ë§¤", "ë›°ì–´ë‚´", "ë‚  ì£½ì´", "ë‚´ ëª©ìˆ¨"]
    if any(keyword in current_question.lower() for keyword in crisis_keywords):
        crisis_message = "\n\nìœ„ê¸° ìƒí™©ì—ì„œëŠ” ì „ë¬¸ê°€ì˜ ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ìì‚´ì˜ˆë°© í•«ë¼ì¸ìœ¼ë¡œ ì—°ë½í•´ì£¼ì„¸ìš”: 1393 (24ì‹œê°„ ìœ„ê¸°ìƒë‹´ì „í™”)"
        answer += crisis_message

    memory_store[thread_id].append(ChatMessage(role="ai", content=answer))

    return {
        "reply": answer,
        "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
    }

# === [6] í—¬ìŠ¤ ì²´í¬ ===
@app.get("/health")
@app.get("/")
async def health_check():
    return {"status": "ok", "model": gemma_model_name}

# === [7] ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)