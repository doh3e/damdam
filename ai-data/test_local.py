# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch

# # === [1] ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ===

# # ë²¡í„° ê²€ìƒ‰ìš© ì„ë² ë”© ëª¨ë¸
# retriever_model = SentenceTransformer("BAAI/bge-m3")
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ (Gemma 3 4B)
# llm_tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
# llm_model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-3-4b-it",
#     device_map={"": device},
#     torch_dtype=torch.float16
# )

# # FAISS ì¸ë±ìŠ¤ì™€ chunks ë¡œë”©
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)


# # === [2] FAISS ê²€ìƒ‰ í•¨ìˆ˜ ===

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     """ì§ˆë¬¸ ë²¡í„°í™” í›„ FAISSë¡œ ê´€ë ¨ ë¬¸ì„œ top_kê°œ ê²€ìƒ‰"""
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]


# # === [3] í”„ë¡¬í”„íŠ¸ ìƒì„± (RAG + ëŒ€í™”í˜• ìŠ¤íƒ€ì¼) ===

# def build_rag_prompt(query: str, retrieved_docs: list[str]) -> str:
#     """ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ ìë£Œë¡œ ë„£ê³  LLMì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€ ìƒì„±í•˜ë„ë¡ êµ¬ì„±"""
#     context = "\n\n".join(retrieved_docs)

#     prompt = f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

# ì§€ê¸ˆ ì‚¬ìš©ìëŠ” ì‚¶ì— ì§€ì¹˜ê³  ê°ì •ì ìœ¼ë¡œ ë§¤ìš° ë¶ˆì•ˆì •í•œ ìƒíƒœì¼ ìˆ˜ ìˆì–´.  
# ë„ˆì˜ ì—­í• ì€ ë‹¨ìˆœíˆ ìœ„ë¡œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ê°ì •ì„ ì„¸ì‹¬í•˜ê²Œ ìœ ì¶”í•˜ê³ ,  
# ê·¸ ì•ˆì— ìˆ¨ì–´ ìˆëŠ” ìƒì²˜ì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ë³´ëŠ” ê²ƒì´ì•¼.

# ì•„ë˜ì—ëŠ” ë¹„ìŠ·í•œ ê³ ë¯¼ì„ ê°€ì§„ ì‚¬ëŒë“¤ê³¼ì˜ ì‹¤ì œ ìƒë‹´ ì‚¬ë¡€ê°€ ìˆì–´.  
# ì´ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•´ì„œ ì§€ê¸ˆ ì‚¬ìš©ìì—ê²Œ ê¼­ ë§ëŠ” **ìƒˆë¡œìš´ ë‹µë³€**ì„ **ë„ˆì˜ ì–¸ì–´ë¡œ ì§ì ‘** ë§Œë“¤ì–´ì¤˜.
# ë˜í•œ ì‚¬ìš©ìê°€ ì´ì „ ìì‹ ì˜ ìƒë‹´ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë©´ ê¼­ ê¸°ì–µí•´ ë‘ì—ˆë‹¤ê°€, ë‹µë³€í•´ì¤˜.

# ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
# 1. ìƒë‹´ìë‹˜ì˜ ë§ ì†ì—ì„œ ì–´ë–¤ ê°ì •ê³¼ ë°°ê²½ì´ ëŠê»´ì§€ëŠ”ì§€ ë¨¼ì € ìœ ì¶”í•´.  
# 2. ê°ì •ì„ ë¨¼ì € ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³ ,  
# 3. ê·¸ë‹¤ìŒìœ¼ë¡œ ì‹¤ì§ˆì ì¸ ìœ„ë¡œì™€ ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì´ì–´ê°€.  
# 4. ë¬¸ì œì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ì•„ê°€ëŠ” ì§ˆë¬¸ë„ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë˜ì ¸ì¤˜.  
# 5. ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šê²Œ. ì§€ì‹ ì „ë‹¬ë³´ë‹¤ ê´€ê³„ í˜•ì„±ì„ ìš°ì„ í•´.  
# 6. **â€˜ì‚¬ìš°ë‹˜â€™ì´ë¼ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ê³ , ë°˜ë“œì‹œ â€˜ìƒë‹´ìë‹˜â€™ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•´.**  
# 7. ìƒë‹´ì‚¬ë¡œì„œ ë„ˆë¬´ ì§§ì§€ë„, ë„ˆë¬´ ì¥í™©í•˜ì§€ë„ ì•Šê²Œ. ì§„ì‹¬ì´ ëŠê»´ì§€ëŠ” ë¶„ëŸ‰ìœ¼ë¡œ ë‹µë³€í•´.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """
#     return prompt


# # === [4] LLM ì‘ë‹µ ìƒì„± ===

# def generate_rag_response(query: str) -> tuple[str, str]:
#     """RAG ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ êµ¬ì„± + ìƒì„±í˜• ë‹µë³€ (ê²€ìƒ‰ ê²°ê³¼ë„ ë°˜í™˜)"""
#     retrieved_docs = retrieve_relevant_chunks(query, top_k=3)
#     prompt = build_rag_prompt(query, retrieved_docs)

#     inputs = llm_tokenizer(prompt, return_tensors="pt")
#     outputs = llm_model.generate(
#         **inputs,
#         max_new_tokens=1000,
#         temperature=0.7,
#         top_p=0.9,
#         do_sample=True
#     )

#     response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return "\n\n".join(retrieved_docs), response


# # === [5] ì‹¤í–‰ ì˜ˆì‹œ ===

# if __name__ == "__main__":
#     user_query = "ì£½ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ìê¾¸ ë“¤ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
#     print("ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:", user_query)

#     retrieved_text, response = generate_rag_response(user_query)

#     print("\nğŸ“š [ì°¸ê³ ìë£Œ (RAG ê²€ìƒ‰ ê²°ê³¼)]")
#     print(retrieved_text)

#     print("\nğŸ§  [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]")
#     print(response)

# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch

# # === [0] ë””ë°”ì´ìŠ¤ ì„¤ì • ===
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # === [1] ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ===

# # ë²¡í„° ê²€ìƒ‰ìš© ì„ë² ë”© ëª¨ë¸ (sentenceâ€transformers ë„ GPU ì‚¬ìš© ê°€ëŠ¥)
# retriever_model = SentenceTransformer("BAAI/bge-m3", device=device)

# # ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ (Gemma 3 4B) â†’ GPU ë¡œë“œ
# llm_tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
# llm_model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-3-4b-it",
#     torch_dtype=torch.float16  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ FP16
# ).to(device)               # ì „ì²´ ëª¨ë¸ì„ GPUë¡œ ì´ë™

# # FAISS ì¸ë±ìŠ¤ì™€ chunks ë¡œë”©
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [2] FAISS ê²€ìƒ‰ í•¨ìˆ˜ ===

# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_text = f"ì§ˆë¬¸: {query}"
#     query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# # === [3] í”„ë¡¬í”„íŠ¸ ìƒì„± ===

# def build_rag_prompt(query: str, retrieved_docs: list[str]) -> str:
#     context = "\n\n".join(retrieved_docs)
#     return f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼...
# [ì°¸ê³ ìë£Œ]
# {context}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [4] LLM ì‘ë‹µ ìƒì„± ===

# def generate_rag_response(query: str) -> tuple[str, str]:
#     retrieved_docs = retrieve_relevant_chunks(query, top_k=3)
#     prompt = build_rag_prompt(query, retrieved_docs)

#     # í† í¬ë‚˜ì´ì§• & ë””ë°”ì´ìŠ¤ ì´ë™
#     inputs = llm_tokenizer(prompt, return_tensors="pt").to(device)

#     # ìƒì„±
#     outputs = llm_model.generate(
#         **inputs,
#         max_new_tokens=256,
#         do_sample=True,
#         temperature=0.7,
#         top_p=0.9
#     )

#     response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return "\n\n".join(retrieved_docs), response

# # === [5] ì‹¤í–‰ ì˜ˆì‹œ ===

# if __name__ == "__main__":
#     user_query = "ì£½ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ìê¾¸ ë“¤ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
#     print("ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:", user_query)

#     retrieved_text, response = generate_rag_response(user_query)

#     print("\nğŸ“š [ì°¸ê³ ìë£Œ]")
#     print(retrieved_text)

#     print("\nğŸ§  [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]")
#     print(response)

### gpuì— ì˜¬ë¦° ì½”ë“œ -> floatì—ì„œ ì—ëŸ¬ ë°œìƒìƒ
# import json
# import faiss
# import torch
# from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import re

# # === [0] GPU ì„¤ì • ===
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print("Using device:", device)

# # === [1] ëª¨ë¸ & ë°ì´í„° ë¡œë”© ===
# retriever = SentenceTransformer("BAAI/bge-m3", device=device)

# tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
# llm_model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-3-4b-it",
#     torch_dtype=torch.float16
# ).to(device)
# llm_model.config.pad_token_id = tokenizer.eos_token_id

# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json","r",encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [2] ê²€ìƒ‰ í•¨ìˆ˜ ===
# def retrieve_relevant_chunks(query, top_k=3):
#     vec = retriever.encode([f"ì§ˆë¬¸: {query}"], convert_to_numpy=True)
#     faiss.normalize_L2(vec)
#     _, I = index.search(vec, top_k)
#     return [chunks[i] for i in I[0]]

# # === [3] í”„ë¡¬í”„íŠ¸ ìƒì„± ===
# def build_rag_prompt(query, docs):
#     ctx = "\n\n".join(docs)
#     return f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.

# [ì°¸ê³ ìë£Œ]
# {ctx}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [4] ì‘ë‹µ ìƒì„± ===
# def generate_rag_response(query):
#     docs = retrieve_relevant_chunks(query)
#     prompt = build_rag_prompt(query, docs)
#     inputs = tokenizer(prompt, return_tensors="pt").to(device)

#     try:
#         outputs = llm_model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=True,
#             temperature=0.7,
#             top_p=0.9,
#             pad_token_id=tokenizer.eos_token_id,
#             eos_token_id=tokenizer.eos_token_id,
#             repetition_penalty=1.2,
#             no_repeat_ngram_size=2
#         )
#     except RuntimeError:
#         outputs = llm_model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=False,
#             num_beams=4,
#             early_stopping=True,
#             pad_token_id=tokenizer.eos_token_id,
#             eos_token_id=tokenizer.eos_token_id
#         )

#     full = tokenizer.decode(outputs[0], skip_special_tokens=True)

#     # '[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]' ë’¤ ë¶€ë¶„
#     if "[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]" in full:
#         resp = full.split("[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]",1)[1]
#     else:
#         resp = full[len(prompt):]
#     resp = resp.strip()

#     # íŠ¹ìˆ˜ í† í° ì œê±°
#     resp = re.sub(r"<unused\d+>", "", resp)
#     # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ í—ˆìš©
#     resp = re.sub(r"[^\uAC00-\uD7A3\w\s\.,!?\-]", "", resp)
#     resp = resp.strip()

#     return "\n\n".join(docs), resp

# # === [5] í…ŒìŠ¤íŠ¸ ===
# if __name__ == "__main__":
#     q = "ìš”ì¦˜ ë„ˆë¬´ ë¶ˆì•ˆí•´ìš”"
#     docs, answer = generate_rag_response(q)
#     print("[ì°¸ê³ ìë£Œ]\n", docs)
#     print("[ë‹µë³€]\n", answer)

## QT
# test_quant_rag.py
## guff ì‹œë„ í–ˆë˜ ì½”ë“œë“œ
# import os
# import json
# import faiss
# import torch
# import re
# from sentence_transformers import SentenceTransformer
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     BitsAndBytesConfig
# )

# # === [0] ì„¤ì • ===
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# MODEL_PATH = "/home/ubuntu/gemma-3-12b-it"
# OFFLOAD_FOLDER = "/home/ubuntu/offload"

# # === [1] ì„ë² ë”© ëª¨ë¸ (RAG) ===
# print(f"Loading retriever on {DEVICE}â€¦")
# retriever = SentenceTransformer("BAAI/bge-m3", device=DEVICE)

# # === [2] í† í¬ë‚˜ì´ì € & LLM (quantized) ë¡œë“œ ===
# print("Loading tokenizer & quantized LLMâ€¦")
# tokenizer = AutoTokenizer.from_pretrained(
#     MODEL_PATH,
#     trust_remote_code=True
# )

# quant_cfg = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True
# )

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_PATH,
#     quantization_config=quant_cfg,
#     device_map="auto",
#     offload_folder=OFFLOAD_FOLDER,
#     torch_dtype=torch.float16,
#     low_cpu_mem_usage=True,
#     trust_remote_code=True
# )
# model.config.pad_token_id = tokenizer.eos_token_id
# print("Model ready.")

# # === [3] FAISS ì¸ë±ìŠ¤ ë¡œë“œ ===
# print("Loading FAISS indexâ€¦")
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [4] ê²€ìƒ‰ í•¨ìˆ˜ ===
# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     if not query.strip():
#         raise ValueError("ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
#     q = retriever.encode([f"ì§ˆë¬¸: {query}"], convert_to_numpy=True)
#     faiss.normalize_L2(q)
#     _, I = index.search(q, top_k)
#     return [chunks[i] for i in I[0]]

# # === [5] í”„ë¡¬í”„íŠ¸ ìƒì„± ===
# def build_rag_prompt(query: str, docs: list[str]) -> str:
#     ctx = "\n\n".join(docs)
#     return f"""ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.

# [ì°¸ê³ ìë£Œ]
# {ctx}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [6] ìƒì„± í•¨ìˆ˜ ===
# def generate_rag_response(query: str) -> tuple[str, str]:
#     try:
#         docs = retrieve_relevant_chunks(query, top_k=3)
#     except ValueError as ve:
#         return "", f"âŒ ì…ë ¥ ì˜¤ë¥˜: {str(ve)}"

#     prompt = build_rag_prompt(query, docs)

#     inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
#     try:
#         out = model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=True,
#             temperature=0.7,
#             top_p=0.9,
#             pad_token_id=tokenizer.eos_token_id,
#             eos_token_id=tokenizer.eos_token_id,
#             repetition_penalty=1.2,
#             no_repeat_ngram_size=2
#         )
#     except RuntimeError as e:
#         print("Sampling failed, fallback to beam search:", e)
#         out = model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=False,
#             num_beams=4,
#             early_stopping=True,
#             pad_token_id=tokenizer.eos_token_id,
#             eos_token_id=tokenizer.eos_token_id
#         )

#     full = tokenizer.decode(out[0], skip_special_tokens=True)
#     if "[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]" in full:
#         resp = full.split("[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]", 1)[1].strip()
#     else:
#         resp = full[len(prompt):].strip()

#     resp = re.sub(r"<unused\\d+>", "", resp).strip()
#     return "\n\n".join(docs), resp

# # === [7] ì‹¤í–‰ ì˜ˆì‹œ ===
# if __name__ == "__main__":
#     user_query = input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ì‚¬ëŠ”ê²Œ ë„ˆë¬´ í˜ë“¤ì–´ì„œ ì£½ê³ ì‹¶ì–´ìš”.").strip()
#     docs, answer = generate_rag_response(user_query)
#     print("\nğŸ“š [ì°¸ê³ ìë£Œ]\n", docs)
#     print("\nğŸ¤– [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]\n", answer)

### ì–‘ìí™” ì¬ì‹œë„ë„
# import json
# import faiss
# import numpy as np
# import torch
# import re
# from sentence_transformers import SentenceTransformer
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     BitsAndBytesConfig,
# )

# # === [0] ë””ë°”ì´ìŠ¤ ì„¤ì • ===
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {DEVICE}")

# # === [1] ì„ë² ë”© ëª¨ë¸ (RAG) ===
# retriever_model = SentenceTransformer("BAAI/bge-m3", device=DEVICE)

# # === [2] ì–‘ìí™” ì„¤ì • ë° LLM ë¡œë“œ ===
# quant_cfg = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=True
# )

# llm_tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
# llm_model = AutoModelForCausalLM.from_pretrained(
#     "google/gemma-3-4b-it",
#     device_map="auto",
#     quantization_config=quant_cfg,
#     torch_dtype=torch.float16,
#     trust_remote_code=True
# )
# llm_model.config.pad_token_id = llm_tokenizer.eos_token_id
# llm_model.eval()

# # === [3] FAISS ë¡œë”© ===
# index = faiss.read_index("qa_index_bge_m3.faiss")
# with open("qa_chunks.json", "r", encoding="utf-8") as f:
#     chunks = json.load(f)

# # === [4] ê²€ìƒ‰ í•¨ìˆ˜ ===
# def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
#     query_vec = retriever_model.encode([f"ì§ˆë¬¸: {query}"], convert_to_numpy=True)
#     faiss.normalize_L2(query_vec)
#     _, I = index.search(query_vec, top_k)
#     return [chunks[i] for i in I[0]]

# # === [5] í”„ë¡¬í”„íŠ¸ êµ¬ì„± ===
# def build_rag_prompt(query: str, docs: list[str]) -> str:
#     context = "\n\n".join(docs)
#     return f"""
# ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ AI ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼.

# [ì°¸ê³ ìë£Œ]
# {context}

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {query}

# [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
# """

# # === [6] ì‘ë‹µ ìƒì„± ===
# def generate_rag_response(query: str) -> tuple[str, str]:
#     docs = retrieve_relevant_chunks(query)
#     prompt = build_rag_prompt(query, docs)
#     inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)

#     try:
#         outputs = llm_model.generate(
#             **inputs,
#             max_new_tokens=512,
#             do_sample=True,
#             temperature=0.7,
#             top_p=0.9,
#             repetition_penalty=1.2,
#             no_repeat_ngram_size=2,
#             pad_token_id=llm_tokenizer.eos_token_id,
#             eos_token_id=llm_tokenizer.eos_token_id
#         )
#     except RuntimeError as e:
#         print("â— ìƒ˜í”Œë§ ì‹¤íŒ¨. ê·¸ë¦¬ë””ë¡œ ì¬ì‹œë„:", e)
#         outputs = llm_model.generate(
#             **inputs,
#             max_new_tokens=256,
#             do_sample=False,
#             num_beams=4,
#             early_stopping=True,
#             pad_token_id=llm_tokenizer.eos_token_id,
#             eos_token_id=llm_tokenizer.eos_token_id
#         )

#     full = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

#     # '[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]' ê¸°ì¤€ ì¶”ì¶œ
#     if "[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]" in full:
#         resp = full.split("[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]", 1)[1]
#     else:
#         resp = full[len(prompt):]
#     resp = re.sub(r"<unused\d+>", "", resp).strip()
#     return "\n\n".join(docs), resp

# # === [7] ì‹¤í–‰ ì˜ˆì‹œ ===
# if __name__ == "__main__":
#     q = "ìš”ì¦˜ ë„ˆë¬´ ë¶ˆì•ˆí•´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ì£ ?"
#     docs, answer = generate_rag_response(q)
#     print("\nğŸ“š [ì°¸ê³ ìë£Œ]\n", docs)
#     print("\nğŸ§  [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]\n", answer)

### ì–‘ìí™” + í”„ë¡¬í”„íŠ¸ ì–‘ì‹ ë³€ê²½
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# === [1] ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ===

# ë²¡í„° ê²€ìƒ‰ìš© ì„ë² ë”© ëª¨ë¸
retriever_model = SentenceTransformer("BAAI/bge-m3")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ (Gemma 3 4B)
llm_tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
llm_model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-4b-it",
    device_map={"": device},
    torch_dtype=torch.float16
)
llm_model.config.pad_token_id = llm_tokenizer.eos_token_id

# FAISS ì¸ë±ìŠ¤ì™€ chunks ë¡œë”©
index = faiss.read_index("qa_index_bge_m3.faiss")
with open("qa_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)


# === [2] FAISS ê²€ìƒ‰ í•¨ìˆ˜ ===

def retrieve_relevant_chunks(query: str, top_k: int = 3) -> list[str]:
    """ì§ˆë¬¸ ë²¡í„°í™” í›„ FAISSë¡œ ê´€ë ¨ ë¬¸ì„œ top_kê°œ ê²€ìƒ‰"""
    query_text = f"ì§ˆë¬¸: {query}"
    query_vec = retriever_model.encode([query_text], convert_to_numpy=True)
    faiss.normalize_L2(query_vec)
    _, I = index.search(query_vec, top_k)
    return [chunks[i] for i in I[0]]


# === [3] í”„ë¡¬í”„íŠ¸ ìƒì„± (Gemma ê³µì‹ í¬ë§· ì ìš©) ===

def build_rag_prompt(query: str, retrieved_docs: list[str]) -> str:
    """ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ ìë£Œë¡œ ë„£ê³  LLMì´ ê³µì‹ í¬ë§·ìœ¼ë¡œ ë‹µë³€ ìƒì„±í•˜ë„ë¡ êµ¬ì„±"""
    context = "\n\n".join(retrieved_docs)
    instruction = f"ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AI ìƒë‹´ì‚¬ ê°„ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. AIëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³  ì„¬ì„¸í•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.\n\n[ì°¸ê³ ìë£Œ]\n{context}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{query}"

    return (
        f"<start_of_turn>user\n{instruction}<end_of_turn>\n"
        f"<start_of_turn>model\n"
    )


# === [4] LLM ì‘ë‹µ ìƒì„± ===

def generate_rag_response(query: str) -> tuple[str, str]:
    """RAG ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ êµ¬ì„± + ìƒì„±í˜• ë‹µë³€ (ê²€ìƒ‰ ê²°ê³¼ë„ ë°˜í™˜)"""
    retrieved_docs = retrieve_relevant_chunks(query, top_k=3)
    prompt = build_rag_prompt(query, retrieved_docs)

    inputs = llm_tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(device)
    outputs = llm_model.generate(
        **inputs,
        max_new_tokens=1000,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.2,
        no_repeat_ngram_size=2,
        pad_token_id=llm_tokenizer.eos_token_id,
        eos_token_id=llm_tokenizer.eos_token_id
    )

    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

    # "<start_of_turn>model" ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if "<start_of_turn>model" in response:
        response = response.split("<start_of_turn>model")[-1].strip()

    response = re.sub(r"<unused\d+>", "", response)
    return "\n\n".join(retrieved_docs), response


# === [5] ì‹¤í–‰ ì˜ˆì‹œ ===

if __name__ == "__main__":
    user_query = "ì£½ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ìê¾¸ ë“¤ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
    print("ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:", user_query)

    retrieved_text, response = generate_rag_response(user_query)

    print("\nğŸ“š [ì°¸ê³ ìë£Œ (RAG ê²€ìƒ‰ ê²°ê³¼)]")
    print(retrieved_text)

    print("\nğŸ§  [AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]")
    print(response)
