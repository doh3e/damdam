import os
import json
import faiss
import torch
import numpy as np
from typing import List
from dotenv import load_dotenv
from fastapi import FastAPI, Request, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GenerationConfig
from langchain.vectorstores.faiss import FAISS
from langchain.schema import Document
from langchain.docstore import InMemoryDocstore
from langchain.embeddings import HuggingFaceEmbeddings
import re
from collections import defaultdict
import traceback
import time

# === [0] ì„¸ì…˜ ê¸°ì–µ ===
memory_store = defaultdict(list)

# === [1] í™˜ê²½ ë³€ìˆ˜ ë° ëª¨ë¸ ë¡œë”© ===
load_dotenv()

gemma_model_name = "google/gemma-3-4b-it"
print("ğŸ”§ LLM ëª¨ë¸ ë¡œë”© ì‹œì‘")

# ê¸€ë¡œë²Œ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë³€ìˆ˜
tokenizer = None
model = None

# ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
try:
    tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
    
    # âš ï¸ ì¤‘ìš”: CUDA ì»´í“¨íŒ… ëŠ¥ë ¥ í™•ì¸
    if torch.cuda.is_available():
        device_capability = torch.cuda.get_device_capability()
        print(f"CUDA ì»´í“¨íŒ… ëŠ¥ë ¥: {device_capability}")
        
        # í…ì„œì½”ì–´(Tensor Cores) ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸ (Ampere ì•„í‚¤í…ì²˜ ì´ìƒ)
        has_tensor_cores = device_capability[0] >= 8
        
        # ìµœì†Œí•œì˜ ì•ˆì „í•œ dtype ì„ íƒ
        if has_tensor_cores:
            print("âœ… í…ì„œì½”ì–´ ì§€ì› GPU - torch.bfloat16 ì‚¬ìš©")
            dtype = torch.bfloat16
        else:
            print("âš ï¸ í…ì„œì½”ì–´ ë¯¸ì§€ì› GPU - torch.float16 ì‚¬ìš©")
            dtype = torch.float16
    else:
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        dtype = torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        gemma_model_name,
        device_map="auto",
        torch_dtype=dtype
    )
    
    # ===== í•µì‹¬ ìˆ˜ì • ë¶€ë¶„: ë” ê²¬ê³ í•œ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ =====
    def text_generate(prompt, max_new_tokens=1000, use_fallbacks=True):
        # ì…ë ¥ê°’ ìœ íš¨ì„± ê²€ì‚¬
        if not prompt or not isinstance(prompt, str):
            return "ì…ë ¥ í”„ë¡¬í”„íŠ¸ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
        # ìµœëŒ€ í† í° ìˆ˜ ì¡°ì • (ë„ˆë¬´ í¬ë©´ CUDA OOM ë°œìƒ ê°€ëŠ¥)
        max_new_tokens = min(max_new_tokens, 1500)
        
        # í”„ë¡¬í”„íŠ¸ í† í°í™” ë° ë””ë°”ì´ìŠ¤ ì´ë™
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        except Exception as e:
            print(f"í† í°í™” ì˜¤ë¥˜: {e}")
            return "í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ëª…ì‹œì  ìƒì„± ì„¤ì • - multinomial ìƒ˜í”Œë§ ì˜¤ë¥˜ í•´ê²°
        generation_config = GenerationConfig(
            max_new_tokens=max_new_tokens,
            do_sample=False,  # greedy decoding ì‚¬ìš©
            temperature=1.0,  # ëª…ì‹œì ìœ¼ë¡œ ê°’ì„ ì„¤ì • 
            num_beams=1,      # beam search ë¹„í™œì„±í™”
            top_p=1.0,        # ëª…ì‹œì  ì„¤ì •
            top_k=50,         # ëª…ì‹œì  ì„¤ì •
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        # CUDA_LAUNCH_BLOCKING=1 íš¨ê³¼ë¥¼ ë‚´ê¸° ìœ„í•œ ë™ê¸°í™”
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        # ë°©ë²• 1: ì§ì ‘ ìƒì„± (ê¸°ë³¸ ë°©ë²•)
        with torch.no_grad():
            try:
                # ì„¸íŒ… ë°±ì—… ë° ëª…ì‹œì  ì„¤ì • ì ìš©
                old_defaults = {}
                for key, value in generation_config.__dict__.items():
                    if hasattr(model.generation_config, key):
                        old_defaults[key] = getattr(model.generation_config, key)
                        setattr(model.generation_config, key, value)
                
                # ì‹œê°„ ì œí•œìœ¼ë¡œ ìƒì„± ê³¼ì • ëª¨ë‹ˆí„°ë§
                start_time = time.time()
                outputs = model.generate(
                    **inputs,
                    generation_config=generation_config
                )
                generation_time = time.time() - start_time
                print(f"ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
                
                # ì…ë ¥ í† í° ìˆ˜ë¥¼ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
                result = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                
                # ê²°ê³¼ ê²€ì¦
                if not result or result.isspace():
                    raise ValueError("ë¹ˆ ê²°ê³¼ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ì›ë˜ ìƒì„± êµ¬ì„± ë³µì›
                for key, value in old_defaults.items():
                    setattr(model.generation_config, key, value)
                
                print("âœ… ê¸°ë³¸ ìƒì„± ë°©ì‹ ì„±ê³µ")
                return result
                
            except Exception as e:
                print(f"ê¸°ë³¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()
                
                # í´ë°± ì˜µì…˜ì´ êº¼ì ¸ ìˆìœ¼ë©´ ì—¬ê¸°ì„œ ì˜ˆì™¸ ë°œìƒ
                if not use_fallbacks:
                    raise e
                
                # ë°©ë²• 2: íŒŒì´í”„ë¼ì¸ ì‹œë„
                try:
                    print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹œë„...")
                    pipe = pipeline(
                        "text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,
                        temperature=None
                    )
                    result = pipe(prompt)[0]['generated_text']
                    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œê±°
                    if result.startswith(prompt):
                        result = result[len(prompt):]
                    
                    # ê²°ê³¼ ê²€ì¦
                    if not result or result.isspace():
                        raise ValueError("íŒŒì´í”„ë¼ì¸ ìƒì„± ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                        
                    print("âœ… íŒŒì´í”„ë¼ì¸ ë°©ì‹ ì„±ê³µ")
                    return result
                    
                except Exception as e2:
                    print(f"íŒŒì´í”„ë¼ì¸ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                    traceback.print_exc()
                    
                    # ë°©ë²• 3: CPUë¡œ fallback
                    try:
                        print("CPUë¡œ fallback...")
                        # ì¼ì‹œì ìœ¼ë¡œ CPUë¡œ ëª¨ë¸ ì´ë™
                        current_device = next(model.parameters()).device
                        model_cpu = model.to("cpu")
                        inputs_cpu = {k: v.to("cpu") for k, v in inputs.items()}
                        
                        with torch.no_grad():
                            outputs = model_cpu.generate(
                                **inputs_cpu,
                                max_new_tokens=max_new_tokens,
                                do_sample=False,
                                temperature=None
                            )
                        
                        # ë‹¤ì‹œ ì›ë˜ ì¥ì¹˜ë¡œ ë³µì›
                        model.to(current_device)
                        
                        result = tokenizer.decode(outputs[0][inputs_cpu['input_ids'].shape[1]:], skip_special_tokens=True)
                        
                        # ê²°ê³¼ ê²€ì¦
                        if not result or result.isspace():
                            raise ValueError("CPU ìƒì„± ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                            
                        print("âœ… CPU fallback ë°©ì‹ ì„±ê³µ")
                        return result
                        
                    except Exception as e3:
                        print(f"CPU fallbackë„ ì‹¤íŒ¨: {e3}")
                        traceback.print_exc()
                        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
                        return "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„± ì¤‘ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
    print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë§¤ìš° ê²¬ê³ í•œ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©)")
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
    traceback.print_exc()
    tokenizer = None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë³€ìˆ˜ ì´ˆê¸°í™”
    model = None

# === [2] FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ë¡œë”© ===
print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì‹œì‘")
retriever = None
try:
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    index = faiss.read_index("qa_index_bge_m3.faiss")
    with open("qa_chunks.json", "r", encoding="utf-8") as f:
        chunks = json.load(f)
    docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]
    index_to_docstore_id = {i: str(i) for i in range(len(docs))}
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})
    vectorstore = FAISS(
        embedding_model.embed_query,
        index,
        docstore,
        index_to_docstore_id
    )
    # LangChain deprecation ê²½ê³  ìˆ˜ì •
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
    traceback.print_exc()
    retriever = None

# === [3] FastAPI ì•± ì •ì˜ ===
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === [4] ìš”ì²­ ë°”ë”” ì •ì˜ ===
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage] = []
    thread_id: str = "default"

# === [5] ìƒë‹´ ì‘ë‹µ API ===
@app.post("/chat")
async def chat_endpoint(request: Request):
    # 1. ëª¨ë¸ ìƒíƒœ í™•ì¸
    if model is None or tokenizer is None:
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"reply": "AI ëª¨ë¸ì´ í˜„ì¬ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì„œë²„ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.", "sources": []}
        )
    
    # 2. ìš”ì²­ ì²˜ë¦¬
    try:
        raw = await request.json()
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"reply": "ì˜ëª»ëœ ìš”ì²­ í˜•ì‹ì…ë‹ˆë‹¤.", "sources": []}
        )

    # 3. ë©”ì‹œì§€ ì²˜ë¦¬
    try:
        if "message" in raw:
            thread_id = raw.get("thread_id", "default")
            new_message = ChatMessage(role="user", content=raw["message"])
            memory_store[thread_id].append(new_message)
            messages = memory_store[thread_id]
        elif "messages" in raw:
            chat_req = ChatRequest(**raw)
            thread_id = chat_req.thread_id
            memory_store[thread_id].extend(chat_req.messages)
            messages = memory_store[thread_id]
        else:
            return JSONResponse(
                content={"reply": "ìƒë‹´ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ™", "sources": []}
            )

        current_question = messages[-1].content.strip()
        history = messages[:-1]

        # 4. RAG ê²€ìƒ‰
        docs = []
        context = ""
        
        if retriever is not None:
            try:
                # ìƒˆë¡œìš´ invoke ë©”ì„œë“œ ì‚¬ìš© (LangChain ì—…ë°ì´íŠ¸ ë°˜ì˜)
                if hasattr(retriever, "invoke"):
                    docs = retriever.invoke(current_question)
                else:
                    # ì´ì „ ë©”ì„œë“œëŠ” ê²½ê³ ì™€ í•¨ê»˜ ê³„ì† ì‘ë™
                    docs = retriever.get_relevant_documents(current_question)
                    
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œí•œ
                if len(docs) > 3:
                    docs = docs[:3]
                context = "\n\n".join([doc.page_content.replace("ì‚¬ìš°ë‹˜", "ìƒë‹´ìë‹˜") for doc in docs])
            except Exception as e:
                print(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()
                docs = []
                # RAG ê²€ìƒ‰ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

        # 5. ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_text = ""
        # ìµœê·¼ ëŒ€í™” ê¸°ë¡ë§Œ ìœ ì§€ (ì˜ˆ: ìµœê·¼ 3ê°œ)
        recent_history = history[-3:] if len(history) > 3 else history
        for msg in recent_history:
            role = "ìƒë‹´ìë‹˜" if msg.role == "user" else "AI"
            history_text += f"{role}: {msg.content}\n"

        # 6. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë„ˆëŠ” ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³ , ì‹¤ì œ ì‹¬ë¦¬ìƒë‹´ì‚¬ì²˜ëŸ¼ ì„¬ì„¸í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” AIì•¼.

ìƒë‹´í•  ë•ŒëŠ” ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ:
1. ìƒë‹´ìë‹˜ì˜ ê°ì •ì„ ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³  ìœ„ë¡œí•˜ê¸°
2. ë”°ëœ»í•˜ê³  ë°°ë ¤ ê¹Šì€ ë§íˆ¬ ì‚¬ìš©í•˜ê¸°
3. 'ì‚¬ìš°ë‹˜'ì´ë¼ëŠ” í‘œí˜„ì€ ì“°ì§€ ë§ê³  'ìƒë‹´ìë‹˜'ìœ¼ë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•˜ê¸°
4. ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì œê³µí•˜ê¸°

[ì´ì „ ëŒ€í™”]  
{history_text}

[ìƒë‹´ ì°¸ê³  ìë£Œ]  
{context}

[ìƒë‹´ìë‹˜ì˜ í˜„ì¬ ê³ ë¯¼]  
{current_question}

[AI ìƒë‹´ì‚¬ì˜ ë‹µë³€]
"""

        # 7. ì‘ë‹µ ìƒì„±
        start_time = time.time()
        try:
            answer = text_generate(prompt)
            generation_time = time.time() - start_time
            print(f"ì´ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            
            # ì‘ë‹µ ê²€ì¦
            if not answer or answer.isspace() or "ì£„ì†¡í•©ë‹ˆë‹¤" in answer and "ê¸°ìˆ ì  ë¬¸ì œ" in answer:
                raise ValueError("ìœ íš¨í•œ ì‘ë‹µì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            
            # ê¸°ë³¸ ì‘ë‹µ ì œê³µ (ì„œë²„ ì˜¤ë¥˜ê°€ ì•„ë‹Œ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ)
            answer = "ì•ˆë…•í•˜ì„¸ìš”, ìƒë‹´ìë‹˜. í˜„ì¬ AI ì‹œìŠ¤í…œì´ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œë©´ ë” ë‚˜ì€ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤."

        # 8. ìì‚´ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€ ë° ìœ„ê¸° ëŒ€ì‘
        crisis_keywords = ["ìì‚´", "ì£½ê³ ", "ì£½ì„", "ëª©ìˆ¨", "ëª©ë§¤", "ë›°ì–´ë‚´", "ë‚  ì£½ì´", "ë‚´ ëª©ìˆ¨"]
        if any(keyword in current_question.lower() for keyword in crisis_keywords):
            crisis_message = "\n\nìœ„ê¸° ìƒí™©ì—ì„œëŠ” ì „ë¬¸ê°€ì˜ ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ìì‚´ì˜ˆë°© í•«ë¼ì¸ìœ¼ë¡œ ì—°ë½í•´ì£¼ì„¸ìš”: 1393 (24ì‹œê°„ ìœ„ê¸°ìƒë‹´ì „í™”)"
            answer += crisis_message

        # 9. ë©”ëª¨ë¦¬ì— ì‘ë‹µ ì €ì¥
        memory_store[thread_id].append(ChatMessage(role="ai", content=answer))

        # 10. ì„±ê³µ ì‘ë‹µ ë°˜í™˜
        return JSONResponse(
            content={
                "reply": answer,
                "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in docs]
            }
        )
            
    except Exception as e:
        print(f"âš ï¸ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"reply": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.", "sources": []}
        )

# === [6] í—¬ìŠ¤ ì²´í¬ ===
@app.get("/health")
@app.get("/")
async def health_check():
    model_status = "loaded" if model is not None else "not_loaded"
    retriever_status = "loaded" if retriever is not None else "not_loaded"
    
    return {
        "status": "ok", 
        "model": gemma_model_name,
        "model_status": model_status,
        "retriever_status": retriever_status,
        "cuda_available": torch.cuda.is_available()
    }

# === [7] ë©”ëª¨ë¦¬ ê´€ë¦¬ ===
@app.delete("/memory/{thread_id}")
async def clear_memory(thread_id: str):
    if thread_id in memory_store:
        del memory_store[thread_id]
        return {"status": "ok", "message": f"Thread {thread_id} cleared"}
    return {"status": "not_found", "message": f"Thread {thread_id} not found"}

# === [8] ì‹¤í–‰ ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)